\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\lhead{Axel Angel}
\rhead{Properties of Convolutional Neural Networks}
\chead{Report}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\today}

\pdfinfo{
    /Author (Authors)
    /Title (Title)
    /Subject (Subject)
    }

\begin{document}
Write about results, raw sketch to complete.

why chose caffe? because easy, various models available, optimized and flexible.

papers use pca before t-sne, sklearn recommends too, in practice it works well.

some papers found that cnn models are easily fooled by adversial noise, because too linear? because image space too wide?

some papers can train using adversial examples. mnist dataset is not resistant to many generations of adversial training. imagenet works better (because natural images?)

we need a way to formalize invariance, resitance to adversial noise, deformations. present results and compare models, attacks and present numbers.

lenet on mnist can be easily fooled by translations, because dataset is heavily normalized, translation-invariance is weak. small rotations works well but can fool easily. seems invariant to contrast (invariant filters?).

interactive tool findings? discontinuities in the output, we can fool it with certain angles. we see the receptive field by shifting, after some margin it goes non-sense

transfo tracer findings? how to plot better? we find pattern in the t-SNE. Lines for positive angles, a curve for negative angles, does it mean anything since its t-SNE? invariant transformations? blur has small impact, why? shift is important after a certain threshold (margin? recetivity field size?), why? can we relate this to the CNN-code: easy, hard?

train translation-invariance: tried to train lenet with multiple copies of each but shifted of the same amount (fixed ranges), but the accuracy on original test is very low (30\%) probably because it learned positional-dependent features instead of real translation invariance! The shifted test set has 92\% accuracy though.

train translation-invariance: with random translation (among given range), one per axis at most (original + shift x + shift y).

after ensuring my translated training set is correct (high accuracy and so), the t-sne seems ``overcrowded'' or much less separated than before. It's like for eevery digit half is clustered and the rest is spread. Probably one region for centered digit, one spread region for displaced. There are a lot of overlap but there seems to have cluster with the similar translation offset (but different digit!). Lots of clusters grouping similar translation offsets: left-shifted, right-shifted, upper-shifted, lower-shifted. I suspect the network learned there are two/three kind of samples not-shifted and shifted ones (left-right up-down kinds). I should try to have a sample appearing once with x-y-combined translation at random.

with translation-invariance: models keep quite good accuracy on original testset, but lost 2\% compared to original model. Accuracy on shifted testset is much lower (down to 91\%) probably because lots of sample are out of the receptive fields or too much information was lost to disambiguate.

with translation-invariance: t-sne has kind of 5 kind clusters per digit: right'es (top-left), left'es (bottom-left), high'es (bottom-center), low'es (bottom-right), centered digits. Sometimes they intersect (probably due to optimisation, local optimum = couldn't separate). From the centered-digit cluster, they are 4 outgoing directions that's proportional to the distortion (shift) value. These translation clusters converge to kind of the same for all digits.

using the original model, taking a companion point close in t-sne for one distorted point per class (for large rotation, extreme case) we see that t-sne change a lot: sometimes these points are far apart (more samples = more weight in t-SNE to put them far?), moreover we see the original images rotation-distorted form a loop (extreme negative close to extreme positive), often the original image rotation is split (the line disappears and continues far away), continuity is broken for certain small change?
\end{document}
