\documentclass[a4paper,12pt]{report}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{cite}
\usepackage{hyperref}

\linespread{1.2}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\newcommand{\myname}{Axel Angel}
\newcommand{\mytitle}{Towards Distortion-Predictable Embedding of Neural Networks}
\newcommand{\mysubtitle}{Thesis}
\newcommand{\mydate}{\today}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\lhead{\myname}
\rhead{\mytitle{} - \mysubtitle{}}
\chead{}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\mydate}

\pdfinfo{
    /Author (\myname)
    /Title (\mytitle)
    /Subject (\mysubtitle)
    }

\newcommand{\eg}{e.g.}
\setkeys{Gin}{width=0.75\textwidth}

\begin{document}
\thispagestyle{empty}
% TODO: make
First page (make guard, epfl logo, prof names, title, etc)

\newpage
\begin{abstract}
    % TODO: rewrite
    Current research in Computer Vision has shown that Convolutional Neural Networks (CNNs) give state-of-the-art performance in many classification tasks and Computer Vision problems.
    The embedding of CNNs, which is the internal representation produced by the last layer, can indirectly learn interesting topological and relational properties.
    By using a suitable loss function, these models can learn invariance to a wide range of non-linear distortions such as rotation, viewpoint angle or lighting condition.
    In this work, we give useful insights about CNNs embeddings and we propose a new loss function, derived from the contrastive loss, whose mapping under particular distortions is more predictable.
    Our contribution makes a step towards the derivation of features for varying distortions given the features of a single feed-forward pass which is distortion-predictable where usual methods require to feed-forward images under every distortions.
    % ^ TODO: careful about this last statement, too strong!
    We introduce a simple method to fairly compare quantitatively embeddings whose purpose is to capture topological structures of particular distortions.
\end{abstract}

% ==============================================================================
\tableofcontents


% ==============================================================================
\chapter{Introduction}
% FIXME: to write

common use of high-dim data, like images

dim reduction (LLE, isomap, t-SNE), why, drawbacks

an other approach is to use CNN to classify, then dim reduction on embedding

allows to understand what's happening in CNN, better topology

distortions are spread in the embedding

what about learning invariance? but keep information

need mapping (like DrLIM), train with siamese network

predictable mapping << our contribution

need quantitative measures (to compare) << our contribution

new loss function, short summary of results

use case (medical imaging)

\section{Thesis Outline}
% list chapters with their summarzied content


% ==============================================================================
\chapter{Related Work}
% explain each paper, group them and discuss result adv/drawbacks

More than 20 years ago, researchers discovered that {\em Neural Networks} (NN) is a highly flexible model architecture to solve many classification problems.
Most of today models are using {\em Convolutional Neural Networks} (CNN) which are a specialized variant working well for visual tasks like MNIST \cite{mnist_web} and ImageNet \cite{krizhevsky2012imagenet}, face detection as well \cite{rowley1998neural} and many more \cite{prechelt1994proben1}.
This architecture is more efficient for computer vision problems, partly because the convolutions share their weights in the first layers to extract spatial cues.

Current researches is mainly focused on advancing into more complex classification problems using Deep Learning.
This field suggests to improve the current results by deepening the architecture in terms of number of layers to express more abstract and higher-level concepts.
Unfortunately, more layers require more computations than before due to the increase of parameters but researchers started to overcome this challenge.
Ways to speed up the training and the classification appeared and were greatly beneficial for the development of this field in recent years\cite{ciresan2011flexible}\cite{schmidhuber2015deep}\cite{nasse2009face}.
The use of GPUs parallelism and cloud computing allowed scaling up to much deeper architecture (with much more parameters) than before\cite{coates2013deep}.
%paper: Flexible, high performance convolutional neural networks for image classification
Both researchers and professional programmers built various frameworks to train and to use NN with many different goals, like: performance, accessibility or composability.
The most popular are: Theano\cite{bastien2012theano}, Caffe\cite{jia2014caffe}, Torch7\cite{collobert2011torch7}, Pylearn2\cite{goodfellow2013pylearn2}.
We decided to leverage the efficiency and modularity proposed by Caffe for several reasons described later.
%deeper architectures improves accuracy on more complex tasks, people found optimisations to make them fast.

Even though CNN models are now widely used in computer vision with great success thanks to these frameworks, current research exposed our ignorance of these networks behaviors.
Some papers work on reliable ways to fool networks using adverserial attacks which exploits their unintuitive properties\cite{szegedy2013intriguing}.
Most publications don't try to formally justify their good results because it just works. % FIXME: citation needed?
%paper: Intriguing properties of neural networks
Indeed, relations between units are complex due do their non-linear nature, this problem is stressed by distortions present in the inputs as well.
%Moreover the input-output relation, when inputs are kept distorted on purpose, is usually either killing the signal (invariance) or not in a predictable manner (controlled preservation).
It is not possible to directly look at the high-dimensional embedding, because we cannot easily plot nor interpret so many dimensions.
The manifestation of this relation can be looked visually using methods to produce an alternative human-friendly representation.

% moved from dim red chapter
Dimensionality reduction methods is a popular tool in such case\cite{dai2014document}\cite{taylor2011learning} and several methods exist that are specialized for visualization.
Standard methods like PCA, MDS\cite{cox2000multidimensional} for example, are used for reducing dimensions usually as a preprocessing step and they are limited to linearities of the data.
Their primary characteristic is to maximize the variance of the original data which is important for reconstruction but not necessarily for visualization.
Other non-linear dimensionality reduction methods, use an optimisation algorithm with a loss function which improve their flexibility for other purpose like visualization.
They preserve local structure/cluster like: Isomap\cite{tenenbaum2000global}, LLE\cite{roweis2000nonlinear} and SNE\cite{SNE}.
Most of the methods fails to keep global clusters and local details at the same time where as the latest papers show SNE a better potential\cite{SNE}.
%paper: Stochastic Neighbor Embedding

SNE has introduced a better maximization problem to preserve point neighborhood and general clusters with an important emphasis on distances\cite{SNE}.
%Kullback-leibler divergence.
However it suffers from ``center-crowdedness'' and has difficulties in optimisation\cite{t-SNE}.
That's why papers are using t-SNE, a variant of SNE, to reduce CNN embeddings into a 2D human-friendly manifold. % FIXME: more cite?
This method exhibits an easier optimisation formulation and it preserve more structures at diverse scales.
More convincing examples were created with t-SNE directly from raw popular datasets\cite{van2009new} like MNIST\cite{t-SNE}.

It has become possible to easily see many more details in CNN embeddings: how samples are grouped depending on multiple factors: essentially based on similarity of the digits (strokes, thickness and shapes) and natural variance (rotation).
%Although it is unintuitive, this can be seen by inspecting the embedding.
%we can add constraints against distortions directly over the embedding, to cluster them the way we meant.
Many papers use dimensionality reduction to create a human viewable representation of the output embedding \cite{donahue2013decaf}\cite{yu2014visualizing}\cite{yaotiny}.
Only a few papers actually try to formalize the input-output relations\cite{goodfellow2009measuring}.
Some papers proposes to learn transformation-invariant embeddings, by means of modelling distortions directly into their models\cite{gens2014deep} or using data-augmentation\cite{hadsell2006dimensionality}.
%paper: Encoded Invariance in Convolutional Neural Networks
%paper: Deep Symmetry Networks
%proposes an alternative network, symmetry networks, to extend invariance beyond translation by using feature map over arbitrary symmetry groups by use of kernel-based interpolation.
%paper: Dimensionality Reduction by Learning an Invariant Mapping

The latter will be used in our current work as the {\em reference paper} or {\em LeCun}'s works.
It proposes to creates a dimensionality reducing mapping by training a CNN using a new loss function based on energy models, called the {\em contrastive loss}.
Their results using on the MINST dataset\cite{lecun1998mnist} shows that CNN can learn a mapping that distinguish labels, and group alike digits even when they are translated artificially.
They experimented with the NORB dataset\cite{lecun2004learning} as well with intriguing results: each sample is mapped on a 3D cylinder whose axises quantify the orientation in 2D and the azimuth angle in 1D.
The network was successfully trained to ignore lighting conditions, which is a strong non-linear distortions.
They use the Siamese training architecture to present image pairs which are optimized to be close if similar or far otherwise\cite{bromley1993signature}\cite{chopra2005learning}.
There are several advantages of such method: the cost penalty is very low and only present in training, the usage of standard CNN allow more freedom for experiments with different architectures and the proposed loss function is effective and simple.

%Moreover the input-output relation, when inputs are kept distorted on purpose, is usually either killing the signal (invariance) or not in a predictable manner (controlled preservation).
However the input-output relations of distorted images is usually damped instead of being quantified in a predictable way.
We think the distortion information should be kept because they are valuable later on.
Besides, the reference paper provides subjective comments of the embedding coherence (description and figure) but they don't offer nor propose a formal way to measure and compare its quality objectively.
We used this paper as our primary reference for experiments to continue improving their ideas in this work.

In this work, we will present a practical solution towards predictable embedding with respect to distortion and offer a qualitative measure to compare similar methods. % FIXME: review


% ==============================================================================
\chapter{Theory: Neural Networks}
We introduce the general layout of neural networks and their mathematical foundations.
The important differences between {\em Neural Networks} (NN) and {\em Convolutional Neural Networks} (CNN) are discussed and we briefly justify why CNN is so predominant in computer vision tasks.
An intuitive explanation of embeddings is given and will be used later for dimensionality reduction.

\section{NN and CNN Classifier}
%introduce CNN and embeddings

Let us quickly recall the definition of a Neural Network and its workings for classification.
Basically, NN are composed of $L$ layers which are made of inter-connected neurons.
A connection $l_{i \rightarrow j}$ in the $k$th layer is weighted by a parameter $w^k_{i,j}$ which is learned.
Classification is done by feed-feeding the input onto the first layer whose output is propagated layers by layers through the network.
The output of one layer is feed-forwarded onto the next through their neuronal connections.
This process is repeated until the last layer whose output is considered as the network output.
Each layer does a single particular computation over its input.
Usual NN have a repetition of the following pattern: one layer computing inner-products with their neuronal weights then adding biases, followed by a non-linear layer using an activation function.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/NN.jpg}
    \end{center}
    \caption{A Neural Network with two layers -- Source: mechanicalforex.com}
    \label{fig:neural_network}
\end{figure}

The figure \ref{fig:artificial_neurons} presents the computation involving a particular neuron and its activation.
Let us define the $j$th neuron in the $k$th layer for the inner-product case, then its output $z^k_j$ is defined:
\begin{eqnarray}
    z^k_j = \sum_{i=1}^{U^{k-1}} w^k_{i,j} z^{k-1}_i + b^k_j
\end{eqnarray}
where $U^k$ is the number of neurons in layer $k$.
Then the $j$th neuron in the $k$th layer for the activation case:
\begin{eqnarray}
    z^k_j = g(z^{k-1}_j, w^k_{j})
\end{eqnarray}
where $g$ is the activation function with its parameter.
Most of them use the sigmoid activation function and connect each neuron to all the next layer neurons.
Training the weights of such network generally involve gradient descent which minimizes the loss function of the network.
This loss represents the error between the prediction of the network (its output) versus the expected output (in classification: the class label).
The training involves an iterative process where: the input is first feed-forwarded into the network to compute the error, then this latter is back-propagated up to the first layer, while each unit weights is adjusted to minimize the unit error.
This process should be repeated until the error on the validation set has converged to a local optimum.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/800px-ArtificialNeuronModel_english.jpg}
    \end{center}
    \caption{An inner-product neuron with the connection to its activation neuron -- Source: Wikipedia}
    \label{fig:artificial_neurons}
\end{figure}

One particular architecture is the CNN, a special case of NN with certain restrictions.
CNN models are working well practically for many computer vision problems in image classification.
We briefly describe below three main ideas: local receptive fields, weight sharing and subsampling.
In CNN, a convolutional layer can model receptive fields by computing multiple trainable 2D kernels which is convoluted with its entire input whose result is called its feature maps.
We can see a kernel convolution as the replication of a single unit along the dimensions of the input (a 2D grid for images), thus all weights are constrained to be equal by definition.
A convolutional layer contains multiple units where each has its own kernel.
Therefore different kernels are applied to the image where a single kernel convolution is called a feature map.
Such layer naturally computes filters which can be seen like feature extractors driven by data.
The benefit of weight sharing in convolutional layers is to reduce the number of global parameters to learn general purpose filters which can increase generalization.
Usually, CNN have subsampling layers right after convolutional layers to reduce the dimensionality of the feature maps so that more concise and high-level information are extracted.
Most CNN architectures puts multiple convolutional layers connected to the input to extract visual features after which they have regular inner-product layers.
CNNs can be seen as two parts: a trainable feature extractor made by the convolutional network followed by a neural network classifier.
Meanwhile CNN have more constrained, it has been shown they generally outperforms NN with more invariance in multiple computer vision problems \cite{simard2003best}\cite{mnist_web}\cite{lawrence1997face}\cite{krizhevsky2012imagenet}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=\textwidth]{thesis_figures/conv-net2.jpg}
    \end{center}
    \caption{Architecture of a CNN -- Source: code.flickr.net}
    \label{fig:convnet}
\end{figure}

Moreover, researches found interesting structures in the last-layer embedding of classification NN using t-SNE.
The last layer tends to cluster samples of the same class together while separating the rest\cite{donahue2013decaf}\cite{yu2014visualizing}.
This is justified by the fact that deeper layers extract more high-level informations, which is necessary to separate classes, moreover the prediction is a direct product with the last layer, which encourage the network to have a simple structure directly clustered by classes.
Thus it looks reasonable to evaluate the quality of this embedding as a dimensionality reduction method with great potential to capture more information compared to PCA which captures linear correlations.
It is important to note that t-SNE plays an important role in the quality of this 2D embedding: that is why it can already honorably separates MNIST directly.
However, it is important to remember it is not the sole responsible of the better separation using the NN embedding because it has no knowledge of the labels.
The classifier seems to provide better informations to t-SNE which is improving the quality of the structures of the final embedding as we will show later.
%because it learned specialized filters to distinguish the classes.
%Therefore as well as with the help of the classifier features.
This justifies why we combined CNN classifiers with t-SNE to create a 2D embedding for our visualisations.


% ==============================================================================
\chapter{Theory: Dimensionality reduction}
Dimensionality reduction is an important method in machine learning that maps points in a high-dimensional space into a space with a lower number of dimensions.
We will call this lower subspace an {\em embedding}.
Representations suitable for human visualization should keep important relations between points of the original space.
In our experiments, we will apply dimensionality reduction especially on our neural networks' outputs.
This allows us to quickly compare (qualitatively) the impact of controlled distortions applied on images over networks' outputs.
The structure inferred by distances between points such as clusters, intra-cluster neighbors and inter-cluster outsiders reveals important properties of neural network models.
Thus we need accurate low-dimensional representations of embeddings that preserve local distances to compare different models.
In the following sections, we will discuss dimensionality reduction methods that weren't fit for our uses, then we introduce in details the one we used, called t-SNE.

\section{Optimisation Problem}
Various algorithms differentiate themselves by several properties: their goal (\eg: interpolation, compression, visualization), by what they preserve (\eg: variance, distances), how they model point correlation (\eg: linearly or not) or whether they can model new points (\eg: a representation or a mapping).

First, our choice is guided by our primary need, which is to visualize our data.
Thus the method should produce a 2D or 3D embedding, preferably in 2D for scatter plots easily interpretable.
Secondly, we need a guarantee on the preservation of relations between points, especially the distances keeping them clustered or not.
Considering the previous works we discussed above, t-SNE is the best candidate in the current state-of-the-art.
We now define the method objective functions, probabilities and explain intuitive properties.

%introduce theory of SNE, then t-SNE modification.
Let us define our dataset $D$ formed by points in the input-space $X$ of dimension $N$: $D = \left\{ x_1, x_2, \dots, x_n \right\}$, each $x_i \in \R^N$, and one representation $D^\ast$ in the output-space $Y$ of dimension $M$: $D^\ast = \left\{ y_1, y_2, \dots, y_n \right\}$, each $y_i \in \R^M$.
The process of dimensionality reduction is to find the best representation $D^\ast$ that best preserves the most ``important'' information between $x_i$ and $y_i$ for each point.
SNE uses two Gaussian distributions for each point expressing the neighbor distances in $X$ and its equivalent in $Y$.
The Kullback-Leibler divergence is used to compute the objective function, which represents the mismatch of these two distribution for each pair:
\begin{eqnarray}
    L = \sum_{i=1}^n KL(P_i || Q_i) = \sum_{i=1}^n \sum_{j=1}^n p_{j|i} \log\left(\frac{p_{j|i}}{q_{j|i}}\right)
\end{eqnarray}
The probability for a pair of point $x_i$ electing $x_j$ in $X$ follows a Gaussian is as follows:
\begin{eqnarray}
    p_{j|i} = \frac{\exp(-|x_i - x_j|^2 / 2 \sigma_i^2)}{\sum_{k \not = i} \exp(-|x_i - x_k|^2 / 2 \sigma_i^2 )}
    %&
    %q_{j|i} = \frac{\exp(-|y_i - y_j|^2)}{\sum_{k \not = i} \exp(-|y_i - y_k|^2)}
\end{eqnarray}
The equivalent probability $q_{j|i}$ in $Y$ is the same except that $\sigma = 0$ and $x$ is replaced by $y$.
Therefore, a closer pair implies a higher neighbor-election probability because the distance is low.
This cost function gives an asymmetrical importance to the distances: nearby points in $X$ are greatly penalized if they are far in $Y$; whereas a small cost is incurred for pairs far in $X$ but close in $Y$.
%There is a simple physical interpretation of the SNE optimisation problem: pairs are modeled by asymmetrical springs in a mechanical system, the best solution is when the system is still.

As we said SNE suffers from crowdedness problems in the middle of the embedding and the optimisation is harder due to the asymmetrical nature of the objective function.
Both problems were addressed in t-SNE which give very good results in practice.
The two major differences with t-SNE is the symmetrization of the cost function and replaces the distributions of the embedding by student variants.
In t-SNE, a single objective function is minimized:
\begin{eqnarray}
    L = \sum_{i=1}^n KL(P || Q) = \sum_{i=1}^n \sum_{j=1}^n p_{ij} \log\left(\frac{p_{ij}}{q_{ij}}\right)
\end{eqnarray}
where:
\begin{eqnarray}
    p_{ij} = \frac{p_{j|i} + p_{i|j}}{2 n}
\end{eqnarray}
which forces outliers points to contribute more to the loss.
And:
\begin{eqnarray}
    q_{ij} = \frac{1 / (1 + |y_i - y_j|^2)}{\sum_{k \not = l} 1/(1 + |y_k - y_l|^2)}
\end{eqnarray}
which replaces the $q$ distribution in SNE by a Student with a heavier tail: distances in high dimensional spaces spread across more dimensions; in low dimensional space, the accurate equivalent distance needs to be much higher per dimension (thus more points end up farer in $Y$, a heavier tail than in $X$).
As before, a point cannot elect itself: $p_{ii} = q_{ii} = 0$ and probabilities are symmetric for both distributions: $p_{ij} = p_{ji}$ and $q_{ij} = q_{ji}$.
As stated previously the input space $X$ has much more dimensions were distances can be expressed than the 2 dimensions of $Y$.
In summary, t-SNE uses Kullback-Leibler divergence to minimize the mismatch of these two spaces by means of probabilities, therefore the chance of important local structures (frequent patterns) being preserved is higher than with other methods (mosts don't express this goal through an objective function).
Global structures is encouraged by the coherence of the local structures as the divergence decreases and the system stabilizes.

\section{CNN for Dimensionality Reduction}
Usual dimensionality reductions like t-SNE are helpful for many visualizing tasks but it has important drawbacks as well.
We think the most important ones are: the computational cost, the incapacity of mapping new points and the indirect control over the resulting embedding.
Current implementations of t-SNE are still rare, unpolished and require tricks to make them tractable in practice (\eg: reducing first with PCA).
Moreover, there is currently no way with t-SNE to map new points (not in $D$) without optimizing the whole system from scratch.
Besides, t-SNE parameters like perplexity and learning rate are not simple to chose.
Fortunately there are new promising alternatives emerging directly harnessing the power of NN.
We introduce such models in the following section.

%there is a direct way to learn an embedding with CNN without t-SNE.
CNN are mostly used for classification but we can optimize them for other purposes as well.
Instead of the softmax loss function used for classification, we can use a special training architecture with a suitable loss that when optimized tries to satisfy some constraints.
For example to create an embedding with chosen properties directly found into the output layer.
Moreover the ideas presented by reduction methods like t-SNE can be formulated in different terms to be applicable to NN.
In our case the most important idea is to keep similar points together and dissimilar far.

The Siamese network combined with a contrastive loss is a good practical solution to train such networks\cite{bromley1993signature}\cite{chopra2005learning}.
Let us define $G_W$, the function that computes the network output, with parameters $W$.
Then the Siamese network put two weight-sharing instances of $G_W$ side by side, each having their own separate input.
This architectural idea is to create two instances of $G_W$ where their weights $W$ are shared.
On top of this Siamese network is placed the ``cost module'', the contrastive loss, which will compute a loss proportional to the difference between the two output.
The complete network takes a pair of images as input, each image fed into a single $G_W$ instance, and the output is computed over their outputs.
When the network is used, only a single instance $G_W$ is required to feed an input which gives the dimensionality-reduced point.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{thesis_figures/siamese_network.jpg}
    \end{center}
    \caption{Schematic representation of a Siamese network -- Source: cs.nyu.edu}
    \label{fig:siamese_network}
\end{figure}

The contrastive loss function is based on energy models but a single attractive term is not sufficient because it would allow degenerate solutions where all points squashed together; thus an opposing term to push dissimilar pairs should appear as well.
The definition of the contrastive loss for a pair is as follows:
\begin{eqnarray}
    L = \frac{1}{2} Y (D_W)^2 + \frac{1}{2} (1-Y) \max(0, m - D_W)^2
\end{eqnarray}
where $Y \in \{0,1\}$ is the label: $1$ for similar pairs, $0$ otherwise; $D_W \in \R^N$ is the difference between the two network outputs and the parameter $m \in \R$ defines the minimal distance between dissimilar points.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{thesis_figures/contrastive_spring.jpg}
    \end{center}
    \caption{A schematic representation of the contrastive loss using physical springs: (a) Shows the points connected to similar points with {\em attract-only} springs. (b) The loss function and its gardient associated with similar pairs. (c) The point connected only with dissimilar points inside the circle of radius $m$ with {\em m-repulse-only} springs. (d) Shows the loss function and its gradient associated with dissimilar pairs. (e) Shows the situation where a point is pulled by other points in different directions, creating equilibrium -- Source \cite{hadsell2006dimensionality}}
    \label{fig:contrastive_spring}
\end{figure}

With this cost function, a label describes whether or not a pair should be close in the embedding.
The definition of similarity is left open for the application at hand.

\section{CNN for Predictable Reduction}
The method above can learn an embedding which tries to pair the way it was intended, but there is still room for improvement.
First, the structure of the resulting embedding is determined by the dataset and again we indirectly shape it by giving pairing information which helps to constraint local structures.
However, there is no guarantee that this system will converge to an intended global structure, for example having certain deformations in a predictable ordering (\eg: from lowest to highest) or shape (\eg: a line, plane or circle).
Secondly, the training required to make the embedding may converge to a desirable structure after a long time or never even with multiple run using different seeds due to the lack of constraint.
The number of embedding dimensions, $M$, is higher than the dimension of the pairing, $1$, which is why such model is not predictable, because these dimensions gives more freedom to the model in the way to represent these pairs arbitrarily.

In the intend to improve this solution, a possibility is to add more constraints directly into the optimisation process.
We propose to give more informations for each pair which can be used in the loss function to create shape and ordering enforcements.
This method allows to control separately the usage of each dimension of the embedding to express different properties of our dataset.
To achieve this goal, we generalize the contrastive loss to pairs having $p$ pairing labels represented into an $M$-dimensional embedding, where $p \leq N$ by definition.
The relation between a pairing label and its binds to certain dimensions, should be defined per a use-case basis, as the pair relation.

We now introduce our loss function with a formal definition.
Let us define $M$ as the number of embedding dimensions, $p$ the dimensions of the labels, where $p \leq M$ as before.
We define $D \in \N_+^p$ as the number of embedding dimensions assigned for each label.
We restrain ourself to problems where each dimension is assigned to a single label and no dimension is unconstrained: $\sum_i^p D_i = M$.
The definition of the generalized $p$-dimensional contrastive loss for a pair is as follows:
\begin{eqnarray}
    L = \frac{1}{2} \sum_{i=1}^p \left( Y_i (D_{Wi})^2 + (1-Y_i) \max(0, m_i - D_{Wi})^2 \right)
\end{eqnarray}
where $Y \in \{0,1\}^p$ with $Y_i$ being the $i$th component of $Y$, $D_{Wi} \in \R^{D_i}$ is the difference between the two points in the sub-embedding for dimensions of the $i$th component, and $m_i \in \R$ is the minimal distance for dimension $i$.


% ==============================================================================
\chapter{Methodology and Results}

In this chapter, we will describe the general environment we used to produce our experiments, the settings to reproduce them and their results.
The different models we used are first introduced in details with their architecture.
Followed by the explanation of our implementation using Caffe and Python.
We quickly discuss how our datasets are generated based on MNIST and NORB.
We present what we use to quantitatively measure our results that we use later for comparisons with prior art.

\section{Models and parameters}

%explain our models.
As previously said, we are closely following the work of our reference paper \cite{hadsell2006dimensionality}.
Therefore, We employed the two different models they choose to experiment with the two popular datasets MNIST and NORB, one model per dataset.
The first model is {\bf LeNet 5}, a multi-layer neural networks that features an architecture specialized for handwritten characters.
It is with no surprise that they are using it for MNIST, which is a handwritten digit dataset.
We are using a very close fine-tuned variant with minor changes.
This network architecture is comprised of a total of 7 layers with trainable parameters.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/siamese_cnn.jpg}
    \end{center}
    \caption{Architecture of the CNN (based on LeNet 5) -- Source: \cite{hadsell2006dimensionality}}
    \label{fig:siamese_cnn}
\end{figure}

The first part of this network is comprised of two pairs of convolution-pooling layers, where convolution kernels share their weights.
As explained earlier, each convolution is applied to the entire image to create a single feature map.
The first convolutional layer has 20 different trainable $5 \times 5$ kernels, which is followed by a trainable $2 \times 2$ max-pooling layer (with a stride of $2$).
The second convolutional layer has 50 trainable $5 \times 5$ kernels, followed again by a $2 \times 2$ max-pool layer (stride $2$).
The convolutional network ({\em convnet}) part, made by these two pairs of layers, are {\em feature extractors} for this network, it will extract features related to high-level cues to detect digits (\eg: straight or curved strokes).

The second part of this network is composed of two fully-connected layers where only the first one is followed by a non-linear layer.
The first layer has 500 trainable units computing an inner-product, followed by a non-trainable Rectified Linear Unit (ReLU) activations\cite{nair2010rectified}, followed by 10 trainable output units computing an inner-product which gives the digit class (1 versus rest).

The second model is composed of only two fully-connected inner-product layers.
This network is much simpler than the one above because it is trained on a subset of the NORB dataset, which exhibits very few variability (compared to MNIST).
The two layers are made of 20 and 3 trainable units respectively, without non-linearity between them. % FIXME: non-linearity, really?

In the following experiments, we will train our variant of LeNet in two different ways: for digit classification to analyze the ``natural'' embedding and with a Siamese training architecture with the contrastive loss function to analyze a ``constrained'' embedding.
For our second model, we will only train a ``constrained'' embedding with the Siamese architecture for NORB, like in our reference paper.
It is important to note that in the LeNet ``constrained'' experiment we keep only a single fully-connected layer with the number of trainable units equals to the embedding dimensions (\eg: $2$ or $3$).
The motivation is that the classification task is built on top of the network stack generating this embedding but in our second experiment we need this output rawly, like in our reference paper.

In the case of digit classification, the model is trained using Stochastic Gradient Descend (SGD) with a learning rate of 0.01 optimizing a SoftMax loss function.
Siamese models are trained with SGD with a learning rate between 0.01 and 0.001 optimizing the contrastive loss.

For this project we chose the {\bf Caffe} deep-learning framework to train our models.
There are several reasons among them: the simplicity to express networks, train or manipulate them; Caffe includes several network implementations for MNIST with LeNet and Siamese networks, for ImageNet with AlexNet, GoogLeNet where pre-trained versions are available; Moreover Caffe is well optimized C++ with CUDA GPU parallelism, it is flexible including official Python and MatLab bindings and its community is very active and helpful.

For our experiments on MNIST we could easily adapt the provided LeNet to match the architecture design discussed above.
For the second non-convolutional network, we wrote the network definition ourself due to the triviality of this task.
The training stage need many special parameters related to the solver (SGD): learning rate, batch size, momentum, weight decay (gamma and power) and number of epoch.
They were kept unchanged because the Caffe community already fine-tuned them.
We limited all our experiments to $10'000$ iterations, with a batch size of $64$.
All our networks were trained using the Caffe built-in solver provided with the binary using {\tt caffe train}.

% FIXME: add figures

Most of our experiments were implemented using the Python scripting language: to generate our transformed datasets, to compute quantitative measures and visualization projections.
We created our own set of tools to generate distorted training sets for our t-SNE experiments and to generate the distorted pairs training sets for our two Siamese experiments.
The scikit-learn Python library ({\em sklearn}) now implements most of the standard algorithms for machine learning including the one we used: PCA and t-SNE\cite{pedregosa2011scikit}.
The performance of t-SNE depends heavily on the number of input dimensions and in our experiments we had to apply PCA on our datasets first like other papers suggest\cite{t-SNE} (to compute t-SNE in a reasonable amount of time).
We followed sklearn suggesting to always reduce to $50$ dimensions with PCA before applying t-SNE\footnote{t-SNE documentation page: \url{http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html}}.
% FIXME: make source available?

For the visualization of our resulting embeddings, we developed our own solution based on the web library {\em CanvasJS}.
Its main advantage is to allow to interactively visualize with scatter plots directly from the output of t-SNE or the neural networks.
Several features were already provided: coloring points, fast plotting for interactivity; But some were implemented ourself: zooming and moving the viewport, display the image of any sample and filtering/highlighting capabilities to our own application.
All this functionalities give us the necessary tools to inspect qualitatively the results so that at the end we possess the insights we needed.

\subsection{Datasets}
%introduce datasets: mnist and norb.
To perform our experiments, we used two different datasets: {\bf MNIST}\cite{lecun1998mnist} (popular handwritten digit dataset) and {\bf NORB}\cite{lecun2004learning} (popular figures photography dataset).
We briefly present each dataset with some sample images, we give a few insights about their variability and their usual usage in computer vision.

The {\bf MNIST} dataset ({\em modified National Institute of Standards and Technology}) is a gathering of multiple databases of handwritten digits.
One of the goal of MNIST is to provide a unified benchmark for digit recognition and it is widely used in Computer Vision for many years.
It is composed of $60'000$ training and $10'000$ testing images of digits between $0$ and $9$.
The samples are heavily post-processed: uniform black background, white-shaded digits with bold strokes.
The digits are centered such that the gravity center is in the middle and size-normalized so one digit lies inside a restrained sub-region of the $28 \times 28$ bitmap.
The variability of this dataset lies in the different strokes, different writer styles, natural rotations, thickness, curve roundedness and such.
The MNIST dataset is considered nowadays extremely simple, like a toy example due to the lack of natural variability and because state-of-the-art achieved extremely good results.
However it is still a good subject of experiments to try and validate new ideas easily as it is in our case.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/mnist.jpg}
    \end{center}
    \caption{A few samples taken in the training set of MNIST}
    \label{fig:mnist}
\end{figure}

The {\bf NORB} dataset ({\em NYU Object Recognition Benchmark}) is a collection of photos of toy figures taken from a variety of contiguous poses.
This dataset has two variants where we picked the normalized-uniform set.
Its usage is also intended for benchmark, in ``large-scale invariant object categorization''.
It is composed of $48'600$ processed photos of toy figures.
The dataset is made by combining: $9$ elevation views, $18$ azimuth views, $6$ illumination conditions and $5$ toys categories, each containing $10$ toys.
The categories are: humans, animals, airplanes, trucks and cars.
The usual training task is to recognize the toy category whatever viewpoint or illumination is used.
However the reference paper uses only the images of a single plane toy, to infer a representation of the distortions in 3D.
As we are doing comparisons with their methods, we will use the same toy to make our results as well.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/norb.jpg}
    \end{center}
    \caption{A few samples taken from the training set of NORB}
    \label{fig:norb}
\end{figure}

%how we create our train/test dataset
Our experiments require to process the images in a systematic and coherent way.
We are using Python scripts to automatically derive our training and testing set in a predictable way based on the original dataset (MNIST and NORB).
The training architecture apply sequentially SGD over the samples in batch as they appear and it loops over the whole dataset if the number of iterations hasn't reached zero.
The generation of our datasets depends on multiple parameters related to the experiment.
In the case of the t-SNE visualization, we create a distorted dataset containing each sample plus its distorted versions varying in strength, where each image has only a single transformation at a time.
The samples are all distorted using the same set of intensities, quantified as follows: translations and shearing using pixel displacement, rotations using positive and negative angles and blurring with averaging radius.
In the case of Siamese training, we create paired datasets: containing two distorted images with their similarity (1 or 0).
Images in a pair can have different distortions and different classes (digit for MNIST, elevation/azimuth for NORB) and still be paired (label 1), depending on the experiment.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/mnist_transfo_tsne.jpg}
    \end{center}
    \caption{Some examples of distortions in our data-augmented dataset used in our t-SNE visualization experiment}
    \label{fig:mnist_transfo_tsne}
\end{figure}

In the experiments with t-SNE, we are generating a classification task where the training dataset contains one untouched and multiple translated digits with the corresponding label.
This process is called {\em data augmentation}: the dataset is grown artificially by reusing samples but distorted by translations to train invariant models.
Each sample is present with several combinations of increasing translations.
The objective is to train an invariant network and inspect the underlying embedding by using t-SNE.
The goal of this experiments is two folds.
First we would like to understand classifier embeddings and its interaction with t-SNE in a practical manner: testing various data-augmentation methods to see how it impacts the model, its accuracy and how t-SNE adapts.
Secondly, this experiment tests whether we can get an embedding with our prerequisites properties of predictability with a state-of-the-art method specialized for projecting in 2D.

% for siamese
In the Siamese experiments, in order to compare our results to our reference paper, we trained two models based on their settings on MNIST and NORB.
The goal of our reference paper is to train a model where points are close together if they are ``visually'' similar without considering translations.
The implementation of their pairing strategy is as follows.
First they group each sample into a common ``neighborhood'' relation the 5 most similar ones using the euclidean distance in pixel space.
However in practice, neighbors with different digit class would appear and LeCun's didn't mention if they considered legit or not.
In our case we decided to only consider neighborhood with the same class after a visual inspection (we found such pairs enough dissimilar).
The sample and its $n$ translations are all paired with both its 5 neighbors and all its $n$ translations (this sums up to $n \times 5n$ pairs per sample).
All other pairs are considered dissimilar: when the two samples are not part of the same common translated neighborhood.
This strategy uses the regular contrastive loss function in 1D with a single similarity per pair.

Our objective is similar in the sense that we want to stay invariant but only in certain dimensions of our choice.
Our implementation can still quantify distortions by expression them in the other dimensions.
One important advantage is to allow more control, especially in how dimensions are used.
As we said, we are training a single model which is less expensive and allow to learn reusable features for multiple pairings.
%invariance to distortion but our implementation allow to keep a predictable quantification
We can use our extension of the contrastive loss function with $p$ labels for each pair.
The definitions of $p$ and $D$ for the two Siamese experiments are now described and justified in the following.

In the case of MNIST, we are using a 3-dimensional embedding space: where the first two dimensions express neighborhood similarity and the last dimension expresses the distortion similarity.
It is true 2D suffices in this case but to be comparable to LeCun's 2D embedding, we will use a 2D neighborhood space as well and separate distortions into its own space.
Making comparisons between LeCun's 2D space and our model with 1D neighborhood would be unfair due to distances increasing very quickly as the number of dimensions increases.
In this settings, we have: $M=3$ (embedding dimensions), $p = 2$ (two labels) and $D = \left\{ 2, 1 \right\}$ (2D + 1D).
We train our network using a similar pair strategy to LeCun's but with important differences.
Our first label considers similar: a sample and its translations; the sample and its neighbors if they have the same translations; and considers dissimilar: the sample and any non-neighbor sample.
That's why we name it the ``neighborhood'' label.
Our second label considers similar: a sample and any other sample with the same translation; and considers dissimilar any pair without the same translation.
We name it the ``transformation'' label.
Pairs not mentioned in our descriptions above are not included in our datasets.
Moreover we reduce the size and the time for training by randomly taking a subset of dissimilar pairs to balance the label ratio instead of including all of them.

\begin{figure}[h]
    \begin{center}
        \includegraphics{thesis_figures/mnist_pairs.jpg}
    \end{center}
    \caption{Examples of pairs in our Siamese training dataset}
    \label{fig:mnist_pairs}
\end{figure}

In the case of NORB, we are using a 3D embedding as well: where the first 2D are used for the cyclic azimuth (horizontal angle of the viewpoint) and the last 1D for the non-cyclic elevation (vertical angle).
LeCun's work defined similar pairs when the two images are from contiguous elevation or azimuth and we will do the same.
The justification of LeCun's dimension allocation is as follows.
The azimuth viewpoint is cyclic and the shape to represent such structure, without overlapping for different values, is an ellipsis which requires at least 2D.
The elevation viewpoint is not cyclic because NORB has only a smaller subset of values, thus only a single dimension is allocated.
In this case we can directly compare LeCun's embedding and ours as they work on the same problem.
Thus again we have: $M=3$ (embedding dimensions), $p = 2$ (two labels) and $D = \left\{ 2, 1 \right\}$ (2D + 1D).
We train our network based on LeCun's pairing strategy where we separate the azimuth and elevation into two labels (instead of having them merged).

\section{Evaluation Metrics}
evaluations are important to compare.
qualitative gives insights of quality but has drawbacks.
requires human effort, biased and varies among people.
however lecun didn't provide qualitative.
a simple measure is the loss, certain condition.
need to use same params and same test sets.
however don't know exact parameters.
tried to reproduce as best as possible, comparing figures.

in our case, we compare the quality of digit neighbor clustering.
we can use lecun dataset, recall pairing.
recall we project our 3D into 2D to compute loss.
our model not optimizing this directly, disadvantaged.
goal is to see the cost of keeping translation.

need an other measure to compare quality for our goal.
this measure should account for separation and predictability.
% FIXME: to write

%contribution: how we can compute our energy-distance for comparisons.

\section{Results and Discussion}

We conclude this chapter by presenting our results and insights, including the plots and quantitative measures.
Our first experiment is the visualization of the LeNet embedding on the MNIST classification task.
We compare the structural differences between models trained with and without data-augmentation.
However the resulting vector space has a very different structures than one could expect.
Then, we move on with results of directly optimized embedding with models using the contrastive loss.
We apply LeCun's methods by replicating the experiments on MNIST and NORB.
Followed by a throughout detailed discussion of our methods in the same conditions to compare the two.

%tables and graphs: loss of training (for major models), add accuracy for comparison, compare lecun with ours (energy-distance).

\subsection{t-SNE on LeNet}
show t-sne on raw mnist to compare?

train lenet on digit classification task train set.
no data augmentation at this point
give accuracy.
we apply pca followed by t-sne on test set.
digits clearly separated, one cluster per digit.
clusters are coherent, similarity.
most distortions are part of clusters but extremes are outside.

people add distorted samples to their dataset (data augmentation) so models can learn to be transfo invariant, but results shows the embedding cluster them into clusters per transfo instead of per label [XXX].
we can compare to data-augmented model for translation.
give accuracy, show it is translation invariant.
digits are heavily overlapping, not cluster.
clusters are now per translation.
inside each cluster, visible digit separation.
this shows unexpected clustering.
lack of control.
recall tsne is for visualization.

discussion.
give insights: about t-sne, models, clustering, etc.

lenet on mnist can be easily fooled by translations, because dataset is heavily normalized, translation-invariance is weak. small rotations works well but can fool easily. seems invariant to contrast (invariant filters?).

after ensuring my translated training set is correct (high accuracy and so), the t-sne seems ``overcrowded'' or much less separated than before.
It's like for eevery digit half is clustered and the rest is spread.
Probably one region for centered digit, one spread region for displaced.
There are a lot of overlap but there seems to have cluster with the similar translation offset (but different digit!).
Lots of clusters grouping similar translation offsets: left-shifted, right-shifted, upper-shifted, lower-shifted.
I suspect the network learned there are two/three kind of samples not-shifted and shifted ones (left-right up-down kinds).
I should try to have a sample appearing once with x-y-combined translation at random.

shift-invariance training: models with shift-invariance performs better on shifted images obviously at a small cost 2\% on the original test set (not shifted).
The accuracy on the corresponding shifted testset is lower ~91\% but the shifted-trained model is now much better on shifted than the original one, as shown by shifting accuracy histogram.
Tried two ways to train for shift invariance for corresponding datasets: (1) original centered image + 1 x-shifted + 1 y-shifted (2) 3* xy-shifted.
The first version is slightly better than second on the shifting accuracy histogram.
But the second performs slightly better than first on test\_shift4.

with translation-invariance: models keep quite good accuracy on original testset, but lost 2\% compared to original model.
Accuracy on shifted testset is much lower (down to 91\%) probably because lots of sample are out of the receptive fields or too much information was lost to disambiguate.

Justification of data-augmentation (with adversarial): we wanted to be distortion/transformation invariant, and we found that it's not necessarily the case with augmentation.
We need to try other methods: making sure that samples related by label are close together in the vector space of the CNN, distortions shouldn't put them too far apart (wrt to other label).
We can try to use our own loss function (distance) and constraint our vector space directly.
We train our network to have such properties directly.
Then we can try to fine-tune a softmax layer at the output for classification as before but we now have a very different vector space shape (constrained on distances).

\section{Contrastive LeNet}

train siamese network with contrastive on subset train mnist.
show plot embedding on test set.
give loss function, graph?
describe clusters, well separated.
coherent embedding inside clusters.
converge faster?
compare lecun and ours.
give quantitative results.

train siamese network with contrastive on subset norb with plane.
show plot embedding on test set.
give loss function, graph?
describe coherence, continuity, cycle of structure.
converge faster?
compare lecun and ours.
give quantitative results.

general discussion.
with difficulties?

explain what we found using double-contrastive loss with lenet, on mnist.

One of the most important factor for training a Siamese network comes from the pairing method: which samples are considered a pair, which are not.
The results are extremely sensitive to minor changes with regard to pairing distortions, neighborhood samples together and the ordering which can end with important overlapping or incoherent neighborhood.

can add norb, when finished.

usecase of predictable disto


% ==============================================================================
\chapter{Conclusion}

summarize work.

compare to regression, ours easier.

future works.

% bibliography
\bibliography{thesis}{}
\bibliographystyle{plain}

% required by NORB dataset
\nocite{lecun2004learning}

\end{document}
