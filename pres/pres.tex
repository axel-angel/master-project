\documentclass[10pt]{beamer}
\usetheme{Berlin}
\usecolortheme{lily}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{tikz}
\beamertemplatenavigationsymbolsempty

\usepackage{pgfpages}
\setbeameroption{show notes}
\setbeameroption{show notes on second screen=right}

\newcommand{\docauthor}{Axel Angel}
\newcommand{\doctitle}{Towards Distortion-Predictable Embedding of Neural Networks}
\newcommand{\docsubtitle}{Defense}
\newcommand{\eg}{e.g.}
\newcommand{\reff}[1]{~\ref{#1}}
\setkeys{Gin}{width=1.0\textwidth}

\author{\docauthor}
\title{\doctitle \\ \docsubtitle}

\pdfinfo{
    /Author (\docauthor)
    /Title (\doctitle)
    /Subject (\docsubtitle)
    }

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\note{\begin{itemize}
    % Summary (recap what said during presentation)
    \item present an example of application for our method
    \item brief recap of CNN
    \item found t-SNE is good for visualization
    \item not suitable for predictable embeddings
    \item generalized the contrastive loss
    \item provide comparisons in qualitative and quantitative forms
    \item several directions for future works
    \item final thoughts and conclude this work
\end{itemize}}

\tableofcontents{}

\section{Motivations}
\begin{frame}
    \frametitle{Motivations}
    \begin{itemize}
        \item Face recognition
        \item Develop the theory behind it
        \item Our work is a start
    \end{itemize}

    \note{\begin{itemize}
        \item there are examples of applications for our method.
        \item ``face recognition'': pose estimation, expression, mouth, identity
        \item benefit from predictable embeddings.
        \item face recognition is too hard to start
        \item we present simpler with mnist and norb
    \end{itemize}}
\end{frame}

\section{Review of CNNs}
\begin{frame}
    \frametitle{Review of CNNs}
    \begin{itemize}
        \item Architecture
        \item Similar to a blackbox
        \item We focus on the embedding
    \end{itemize}

    \note{\begin{itemize}
        \item architecture:
            \begin{itemize}
                \item built-in feature extractor (ConvNets)
                \item built-in classifier (NeuralNets)
                \item forward/backward SGD
            \end{itemize}

        \item unintuitive properties:
            \begin{itemize}
                \item adverserial attacks
                \item conditions of good convergence
                \item unpredictable embedding
            \end{itemize}
        \item this work focus predictability of embedding under transformations.
        \item can make them learn certain properties
    \end{itemize}}
\end{frame}

\section{Visualization with t-SNE}
\begin{frame}
    \frametitle{Visualization with t-SNE}
    \begin{itemize}
        \item Definition of t-SNE
        \item Optimized properties (spring model)
        \item Previous works
    \end{itemize}

    \note{\begin{itemize}
        \item define t-SNE: reduce dataset from nD to 2D, with optimisation problem.
        \item original points are Gaussian, new points are Student distr.
        \item optimisation make the new repr matches the original: based on points distances
        \item how we did: train/test MNIST, behead last layers and reduce with t-SNE
        \item many papers showed this already but few worked on distortions
        \item kind of distortion: translations, rotations.
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Visualization with t-SNE}
    % FIXME: show figure
    \begin{itemize}
        \item Results are unexpected
        \item We could adapt t-SNE
    \end{itemize}
    However there exist better alternatives.

    \note{\begin{itemize}
        \item discontinuities, questionable clustering
            \begin{itemize}
                \item along some transfo, esp translation
                \item clusters are dominated by distortions (instead of digits)
                \item clusters are mixed up, unclear/confusing
            \end{itemize}
        \item limitations
            \begin{itemize}
                \item lack of control (opti problem is unlabeled)
                \item cannot add points, recompute from scratch, costly
            \end{itemize}
        \item fails to provide concluding results
        \item could reformulate/adapt t-SNE but better alternative exists
        \item what we show in the following
    \end{itemize}}
\end{frame}

\section{Predictable embeddings}
\begin{frame}
    % explain concepts
    \frametitle{Predictable embeddings}
    Learn pair relations directly, by using:
    \begin{itemize}
        \item Neural networks (CNN or NN)
        \item Siamese architecture
        \item Contrastive loss
    \end{itemize}
    %many pairing relations can be learned directly into embeddings.
    % FIXME: show figure
    % FIXME: show formula contrastive loss

    \note{\begin{itemize}
        \item neural networks can express complex non-linear mappings
        \item learn relation: mapping of pairs (2 images) to 1/0 similarity
        \item previous work {\em Dimensionality Reduction by learning an invariant mapping} (DrLIM) by: Raia Hadsell, Sumit Chopra, Yann LeCun
        \item showed: neural networks can learn structured embeddings
        \item showed: can be used for dimensionality reducing
        \item training with: siamese and contrastive for loss (also used for face recognition, same authors)
        \item trained networks to represent: MNIST by similarity in 2D, camera viewpoint in 3D
        \item points similar = close together, similar to t-SNE
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Predictable embeddings}
    In our work we provide:
    \begin{itemize}
        \item An extension of this loss for $N$ relations
            % FIXME: show formula our contrastive loss
        \item Qualitative and quantitative results
    \end{itemize}

    \note{\begin{itemize}
        \item our work is based on the mentioned paper
        \item we reuse the same concepts but extend contrastive loss
        \item our goal is: learn more similarities, separately, add more control
        \item can decide which dimensions express what similarity (align to axes)
        \item plots serve as qualitative results for human to visually compare
        \item but we also introduce quantitative measures to compare with previous work (reproduce DrLIM work)
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Predictable embeddings}
    % FIXME: show our figure + DrLIM figure
    \begin{itemize}
        \item Found it learns multiple relations at once
        \item Found it learns more predictable structures
    \end{itemize}
    \note{\begin{itemize}
        \item % TODO
    \end{itemize}}
\end{frame}

\section{Conclusion}
\begin{frame}
    \frametitle{Future works}
    \begin{itemize}
        \item Make experimental comparisons with regression
        \item Show predictability with auxiliary networks
        \item Apply to the bio-medical imaging field
    \end{itemize}
\end{frame}

% like summary
\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item We gave applications of our work
        \item Then presented our results with t-SNE
        \item Finally offered a alternative with NN
        \item Further directions are promising
    \end{itemize}
\end{frame}

\end{document}
