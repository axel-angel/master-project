\documentclass[a4paper,12pt]{report}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{cite}
\usepackage{hyperref}

\linespread{1.2}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\newcommand{\myname}{Axel Angel}
\newcommand{\mytitle}{Properties of Convolutional Neural Networks}
\newcommand{\mysubtitle}{Report}
\newcommand{\mydate}{\today}

\lhead{\myname}
\rhead{\mytitle{} - \mysubtitle{}}
\chead{}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\mydate}

\pdfinfo{
    /Author (\myname)
    /Title (\mytitle)
    /Subject (\mysubtitle)
    }

\newcommand{\eg}{e.g.}


\begin{document}
\thispagestyle{empty}
% TODO: make
First page (make guard, epfl logo, prof names, title, etc)

\newpage
\begin{abstract}
    % TODO: rewrite
    Current research in Computer Vision has shown that Convolutional Neural Networks (CNNs) give state-of-the-art performance in many classification tasks and Computer Vision problems.
    The embedding of CNNs, which is the internal representation produced by the last layer, can indirectly learn interesting topological and relational properties.
    By using a suitable loss function, these models can learn invariance to a wide range of non-linear distortions such as rotation, viewpoint angle or lighting condition.
    In this work, we give useful insights about CNNs embeddings and we propose a new loss function, derived from the contrastive loss, whose mapping under particular distortions is more predictable.
    Given an input image, only a single forward pass is necessary to generate the outputs of all possible distortions, whereas standard models would require computing all combinations of distortions which is too expensive in practice or invariant models would lose the distortion information.
    % ^ TODO: careful about this last statement, too strong!
    Moreover we introduce a simple method to fairly compare quantitatively embeddings whose purpose is to capture topological structures of particular distortions.
\end{abstract}

% ==============================================================================
\tableofcontents


% ==============================================================================
\chapter{Introduction}
common use of high-dim data, like images

dim reduction (LLE, isomap, t-SNE), why, drawbacks

an other approach is to use CNN to classify, then dim reduction on embedding

allows to understand what's happening in CNN, better topology

distortions are spread in the embedding

what about learning invariance? but keep information

need mapping (like DrLIM), train with siamese network

predictable mapping << our contribution

need quantitative measures (to compare) << our contribution

new loss function, short summary of results

use case (medical imaging)

\section{Thesis Outline}
% list chapters with their summarzied content


% ==============================================================================
\chapter{Related Work}
% explain each paper, group them and discuss result adv/drawbacks

research found NN are good to classify lots of problems.
CNN are naturally more inclined for computer vision (accuracy and speed).

deeper architectures improves accuracy on more complex tasks, people found optimisations to make them fast.

paper: Flexible, high performance convolutional neural networks for image classification

Even though they give impressive results and are widely used in computer vision, we still lack important formal understanding why they work so well.

paper: Intriguing properties of neural networks

CNN are complex and strange results were found.
Research found multiple hypothesises.

% adversial attacks, linearity of networks.
% Towards Deep Neural Network Architectures Robust To Adversarial Examples
% Analysis of classifiers' robustness to adversarial perturbations

It is not possible to directly look into the embedding, due to high dim space.
however people uses dim reduction to create a human viewable representation of this space.
lots of dim reduction exists

paper: Stochastic Neighbor Embedding

proposes a maximization problem that compute from a high-dim dataset a 2D embedding preserving neighborhood distances.
Kullback-leibler divergence.

paper: Visualizing Data using t-SNE
proposes a variation of Stochastic Neighbor Embedding, easier to optimize, produces better 2D map by spreading points more evenly and preserving topological structures

we can see how CNN are clustering samples depending on multiple factors.
people add distorted samples to their dataset (data augmentation) so models can learn to be transfo invariant, but results shows the embedding cluster them into clusters per transfo instead of per label [XXX].
altought it is not intuitive, this can be seen by inspecting the embedding.
we can add constraints against distortions directly over the embedding, to cluster them the way we meant.

paper: Encoded Invariance in Convolutional Neural Networks

encoding invariance in the learnt embedding which preserves the signal.
Formal analysis of impact of rigid motions versus deformations on classification.
Proposing wavelet networks.

paper: Deep Symmetry Networks

proposes an alternative network, symmetry networks, to extend invariance beyond translation by using feature map over arbitrary symmetry groups by use of kernel-based interpolation.

paper: Dimensionality Reduction by Learning an Invariant Mapping

proposes to creates a dimensionality reducing mapping by training a CNN using a new loss function, the contrastive loss, based on energy models.
The results on NORB is a mapping from image space to a 3D cylinder where two axes describes the 2D orientation and the third axe describes the azimuth angle, the mapping is invariant to illumination.
Results on 4s and 9s of MNIST shows that CNN can learn a mapping that separates labels and make similar digits close despite translations.


% ==============================================================================
\chapter{Theory: Dimensionality reduction}
Dimensionality reduction is an important method in machine learning that processes points which are in a high-dimensional space to represent them into a space with a lower number of dimensions, which we call embedding or map.
We need representations suitable for human visualization, by keeping certain relations between points.
In our experiments, we will use dimensionality reduction, mostly on the CNN features in the embedding space, to compare qualitatively relations between distorted inputs and its impact on the outputs, by looking at structures constrained by distances.
Structures represented with great fidelity in maps are necessary to find useful and comparable properties exhibited by different models, and to make educated hypothesises for our researches.
In the following sections, we will discuss dimensionality reduction methods that weren't fit for our uses, then we introduce the one we used, called t-SNE, in details.

\section{Optimisation Problem}
Various algorithms differentiate themselves by several properties: their goal (\eg: interpolation, compression, visualization), by what they preserve (\eg: variance, distances), how they model point correlation (\eg: linearly or not) or whether they can model new points (\eg: a representation or a mapping).

First, our choice is guided by our primary need, which is to visualize our data.
Thus the method should produce a 2D or 3D embedding, preferably in 2D for scatter plots easily interpretable.
Secondly, we need a guarantee on the preservation of relations between points, especially the distances keeping them clustered or not.

% FIXME: maybe better moved into related works
take PCA, MDS as example, only linearity between points, goal is not to keep distances but variance (compute the linear projection that maximize original data variance, keep far point far).
state of art showed other non-linear methods outperform pca easily for visalisuation.
goal differs from ours.

among non-linear dimensionality reduction methods, use an optimisation algorithm with a loss function.
they preserve local structure/cluster like: isomap, LLE, SNE, (and more).
most fail to keep global clusters and local details at once.
SNE has presented a better optimisation problem to preserve locality\cite{SNE}.
but SNE suffers from: center-crowdedness and difficult optimisation.
t-SNE is introduced as a better optimisation formulation and preserve structure at multiple scale, and shown more convincing examples, for example on MNIST\cite{t-SNE} and similar datasets\cite{van2009new}.
moreover other papers employed t-SNE as well to visualize nn embeddings \cite{donahue2013decaf}\cite{yu2014visualizing}\cite{yaotiny}.

introduce theory of SNE, then t-SNE modification.
let's define input space $X = \left\{ x_1, x_2, \dots, x_n \right\}$ then the goal is to map into a lower-space $Y = \left\{ y_1, y_2, \dots, y_m \right\}$, where $n$ is very high (around a few hundreds to thousands, \eg: mnist pixels dimensions: 784) and $m$ is $2$ or $3$.
SNE models each point similarity by expressing the pair-wise distances into neighborhood probabilities.
in a pair, the probability is the gaussian distribution modeling the first point electing the second as its neighbor.
which means the more closer this pair is, the smaller the pair-wise distance will be, thus the higher the neighborhood probability will be.
SNE models two distributions for each pair: a gaussian of the two points in the n-dim input space and a gaussian of their representations in the m-dim embedding.
SNE uses the Kullback-Leibler divergence, which represents the mismatch of these two distribution for each pair, as its cost function for gradient descent optimisation.
This cost function gives an asymmetrical importance to the distances: nearby points in $X$ are greatly penalized if they are far in $Y$; whereas a small cost is incurred for pairs far in $X$ but close in $Y$.
%There is a simple physical interpretation of the SNE optimisation problem: pairs are modeled by asymmetrical springs in a mechanical system, the best solution is when the system is still.
The two major differences with t-SNE is the symmetrization of the cost function and replaces the distributions of the embedding by student variants.

introduce equations here?

\section{NN and CNN Classifier}
introduce CNN and embeddings

nn and cnn are composed of inter-connected layers of units, or neurons.
to classify an input we fed the first layer of the network.
one layer does a single particular computation over its input, usually by adding a bias then followed by a non-linear function, which is then propagated through its connections to the next layer.
the feedforwarding continues until we reach the last layer which is the output of the network.
usual nn are characterized by: neuron computation is a simple inner product between its input and its own weight followed by a sigmoid; each unit is fully connected to all units in the next layer.
training the weights of such network generally involve gradient descent which minimizes the loss function of the network.
This loss function gives the error between the prediction of the network, its output versus the expected output, which is in classification the class label.
To train, the input is first forwarded into the network to compute the error, then this latter is back-propagated up to the first layer, while each unit weights is adjusted to minimize the unit error.
This process should be repeated until the error on the validation set has converged to a local optimum.

cnn is a special case of nn by adding certain restrictions, working well experimentally with computer vision problems such as image classification.
Three main ideas differentiate them: local receptive fields, weight sharing and subsampling.
in cnn, a convolutional layer can model receptive fields by computing multiple trainable 2D kernels which is convoluted with its entire input whose result is called its feature maps.
We can see a kernel convolution as the replication of a single unit along the dimensions of the input (a 2D grid for images), thus all weights are constrained to be equal by definition.
a convolutional layer contains multiple units where each has its own kernel, thus there are different kernels applied to the image, where a single kernel convolution is called a feature map.
Such layer naturally computes filters which can be seen like feature extractors driven by data.
the benefit of weight sharing in convolutional layers is to reduce the number of global parameters to improve generalization.
usual cnn add a subsampling layer right after a convolutional layer to reduce the dimensionality of the feature maps.
moreover most cnn architectures connects convolutional-subsampling layers to the input which extract features, which is then feed into a regular nn.
Thus we can see a cnn as two parts: a trainable feature extractor which is fed into a processing nn.
meanwhile cnn have more constrained, it has been shown they generally outperforms nn with more invariance in multiple computer vision problems \cite{simard2003best}\cite{mnist_web}\cite{lawrence1997face}\cite{krizhevsky2012imagenet}.

as previously said, researches have shown with t-sne that nn trained for classification have interesting structures in their last-layer embedding.
the choice of the layer is justified by the fact that: deeper layers extract more high-level informations, which is necessary to separate classes, moreover the prediction is a direct product with the last layer, which encourage the network to have a simple structure directly clustered by classes.
that's why this last layer tend to cluster samples of the same class together while separating the rest\cite{donahue2013decaf}\cite{yu2014visualizing}.
thus it is reasonable to evaluate the quality of this embedding as a dimensionality reduction method with great potential to capture more information compared to PCA which captures linear correlations.
it is important to note that t-SNE plays an important role in the quality of this 2D embedding by preserving structures but it should be said also that t-sne cannot separate the original dataset as well as with the help of the classifier features.
Because t-SNE has no knowledge of the labels, it is the classifier who improves the quality of the structures of the final embedding, because it learned specialized filters to distinguish the classes.
This justifies why we combined CNN classifiers with t-SNE to create a 2D embedding for our visualisations.

\section{CNN for Dimensionality Reduction}
because the method above has important drawbacks due to t-SNE, among them is the computational cost, the lack of mapping to visualize new points and the impossibility of controlling the embedding directly.
there is a direct way to learn an embedding with CNN without t-SNE.
instead of using cnn for classification, we can train it to create an embedding with chosen properties directly from the last layer.
t-sne ideas can be reused with cnn.
the most interesting constraint is to keep similar points together and dissimilar far.
fortunately, instead of the softmax loss function used for classification, we can use a special training architecture with a suitable loss that when optimized tries to satisfy these constraints.
One solution is the siamese network with a contrastive loss which is used only for the optimisation phase.
The siamese architectural idea is to create two weight-sharing instances of a network where a pair of two samples are fed at once, one per network.
The distance between the two outputs is computed to compute the error against the label for the pair.
% FIXME: figure.
the labels now defines whether pairs should be close in the embedding or not, which looks similar to classification problems but the main difference is in the classification
the contrastive loss function is based on energy models to attract similar pairs and repulse dissimilar pairs, where similarity it to be defined by the usage.

when the training is complete, a single instance of this network is used to feed an input for dimensionality reduction.

\section{CNN for Predictable Reduction}
the method above can learn an embedding which tries to pair the way it was intended, but there is still room for improvement.
First, the structure of the resulting embedding is determined by the dataset and again we indirectly shape it by giving pairing information which helps to constraint local structures.
However, there is no guarantee that this system will converge to an intended global structure, for example having certain deformations in a predictable ordering (\eg: from lowest to highest) or shape (\eg: a line, plane or circle).
Secondly, the training required to make the embedding converge to a certain structure may happen after a long time, or multiple run with different seed can be tried, but there is again no such guarantees.
The number of embedding dimensions, $n$, is higher than the dimension of the pairing, $1$, which is why such model is not predictable, because these dimensions gives more freedom to the model in the way to represent these pairs arbitrarily.

In the intend to improve this solution, a possibility is to add more constraints directly into the optimisation process.
We propose to give more informations for each pair which can be used in the loss function to create these enforcements.
This method allows to control separately the usage of each dimension of the embedding to express different properties of our dataset.
To achieve this goal, we generalize the contrastive loss to pairs having $m$ pairing labels represented into an $n$-dimensional embedding, where $m \leq n$ should hold by definition.
The relation between a pairing label and its binds to certain dimensions, should be defined per a use-case basis, as the pair relation.

introduce our loss function

instead of being invariant, we can optimize the embedding to be predictable in certain way, for eg: quantifying one distortion per axis.


% ==============================================================================
\chapter{Methodology}

explain our models

use of Caffe

introduce datasets: mnist and norb.

how we create our train/test dataset

how are models trained in caffe

contribution: how we can compute our energy-distance for comparisons.


% ==============================================================================
\chapter{Results and Discussion}

tables and graphs: loss of training (for major models), add accuracy for comparison, compare lecun with ours (energy-distance).

\section{t-SNE on LeNet}
explain what we found using t-SNE on last layer of lenet, on mnist.

\section{Contrastive LeNet}
explain what we found using double-contrastive loss with lenet, on mnist.

can add norb, when finished.


% ==============================================================================
\chapter{Conclusion}

% bibliography
\bibliography{thesis}{}
\bibliographystyle{plain}

Caffe: Convolutional Architecture for Fast Feature Embedding
proposes a deep-learning framework focusing on CNN, easy experiment, easily modifiable source, reference models (lenet, imagenet), scalable, GPU, active community.

DIY Deep Learning for Vision: a Hands-On Tutorial with Caffe
caffe tutorial

% required by NORB dataset
Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting. IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) 2004

\end{document}
