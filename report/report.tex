\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\lhead{Axel Angel}
\rhead{Properties of Convolutional Neural Networks}
\chead{Report}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\today}

\pdfinfo{
    /Author (Authors)
    /Title (Title)
    /Subject (Subject)
    }

\begin{document}
Write about results, raw sketch to complete.

why chose caffe? because easy, various models available, optimized and flexible.

papers use pca before t-sne, sklearn recommends too, in practice it works well.

some papers found that cnn models are easily fooled by adversial noise, because too linear? because image space too wide?

some papers can train using adversial examples. mnist dataset is not resistant to many generations of adversial training. imagenet works better (because natural images?)

we need a way to formalize invariance, resitance to adversial noise, deformations. present results and compare models, attacks and present numbers.

lenet on mnist can be easily fooled by translations, because dataset is heavily normalized, translation-invariance is weak. small rotations works well but can fool easily. seems invariant to contrast (invariant filters?).

interactive tool findings? discontinuities in the output, we can fool it with certain angles. we see the receptive field by shifting, after some margin it goes non-sense

transfo tracer findings? how to plot better? we find pattern in the t-SNE. Lines for positive angles, a curve for negative angles, does it mean anything since its t-SNE? invariant transformations? blur has small impact, why? shift is important after a certain threshold (margin? recetivity field size?), why? can we relate this to the CNN-code: easy, hard?

train translation-invariance: tried to train lenet with multiple copies of each but shifted of the same amount (fixed ranges), but the accuracy on original test is very low (30\%) probably because it learned positional-dependent features instead of real translation invariance! The shifted test set has 92\% accuracy though.

train translation-invariance: with random translation (among given range), one per axis at most (original + shift x + shift y).

after ensuring my translated training set is correct (high accuracy and so), the t-sne seems ``overcrowded'' or much less separated than before. It's like for eevery digit half is clustered and the rest is spread. Probably one region for centered digit, one spread region for displaced. There are a lot of overlap but there seems to have cluster with the similar translation offset (but different digit!). Lots of clusters grouping similar translation offsets: left-shifted, right-shifted, upper-shifted, lower-shifted. I suspect the network learned there are two/three kind of samples not-shifted and shifted ones (left-right up-down kinds). I should try to have a sample appearing once with x-y-combined translation at random.

with translation-invariance: models keep quite good accuracy on original testset, but lost 2\% compared to original model. Accuracy on shifted testset is much lower (down to 91\%) probably because lots of sample are out of the receptive fields or too much information was lost to disambiguate.

with translation-invariance: t-sne has kind of 5 kind clusters per digit: right'es (top-left), left'es (bottom-left), high'es (bottom-center), low'es (bottom-right), centered digits. Sometimes they intersect (probably due to optimisation, local optimum = couldn't separate). From the centered-digit cluster, they are 4 outgoing directions that's proportional to the distortion (shift) value. These translation clusters converge to kind of the same for all digits.

using the original model, taking a companion point close in t-sne for one distorted point per class (for large rotation, extreme case) we see that t-sne change a lot: sometimes these points are far apart (more samples = more weight in t-SNE to put them far?), moreover we see the original images rotation-distorted form a loop (extreme negative close to extreme positive), often the original image rotation is split (the line disappears and continues far away), continuity is broken for certain small change?

even after rerun different seed closest1b (to extreme rotation) test1 t-sne, we see occasionally one gap of the curve for rotation. Pairs test1 and closest are still well separated (except for: 0, 7, 9, 3, 6).

get the closest point to the original digit (apply t-sne), we see the pairs are closer to each other but the original may not be close this time. An other part of the curve may be close/match (rotation at different stage). Conclusion: taking close pair of point, we cannot infer that all distorted versions will be close to the corresponding pair, because more points = more weights, t-SNE will try to separate them more if necessary. (close: 1,7,8,2; ok: 0,3,4,6,9)

shift-invariance training: models with shift-invariance performs better on shifted images obviously at a small cost 2\% on the original test set (not shifted). The accuracy on the corresponding shifted testset is lower ~91\% but the shifted-trained model is now much better on shifted than the original one, as shown by shifting accuracy histogram. Tried two ways to train for shift invariance for corresponding datasets: (1) original centered image + 1 x-shifted + 1 y-shifted (2) 3* xy-shifted. The first version is slightly better than second on the shifting accuracy histogram. But the second performs slightly better than first on test\_shift4.

shift-invariance training: shift-invariance models perform as good as the original one as rotation. Intuitively we could think translation-invariance requires many units to encode at the expense of other features (such as natural rotation) but it's not. It probably means that the network even with translation-invariance has more capacity than it looks, we can probably train more for other invariance (like rotation directly) without decreasing significantly the accuracy. It's very easy to understand why considering the quality of the MNIST dataset (very clean, centered, sharp samples without background).

shift-invariance training: my first way to measure the accuracy of such model was biased towards shift3 (i+x+y) because I was only taking image on the x or y-axis without combination, thus shift3 was higher than other which were trained for random xy-shifts (not fair!). Thus I've done other measure based on combinations of xy shift in a grid-like manner. This time we clearly see shift3 is not the best. We can sense a relation between the number of augmented samples and the accuracy on such test. It seems an optimal number of such sample exists because the accuracy augments then decrease (marginally) as the number of sample increases. Thus we think the benefit of adding more samples may be useless or worse detrimental to the accuracy after a certain threshold (here 3*xy). However shift3 still outperforms orig by a large margin (double: ~15->30\%). Comparing data-augmented training, the margin is quite small, even between i+x+y and 2*xy. This probably means the model got a significant amount of information of such augmentation but the rest is random noise (high variance, need to use t-test).
We note that xyr (shift6) is a much harder problem (with only 1 sample) but still, the model already outperforms orig in these conditions, which means CNN can learn to be resitent/invariant to such random distortions. The relative loss of shift6 on the original testset is a bit high (~82\%) but it can probably be improved with more samples.

rotate-invariance training: first we see that the original model was already pretty good, it's the best here. probably because dataset already presents a wide variety of rotations naturally (that's the major the variation among the dataset, aside: thickness, minor changes). Thus even by data-augmenting for shift+rotation, it doesn't outperform the original model, probably because it used too many units to encode shift-invariance. (try train only rotation). That's even more evident as we increase the number of shifted-sample for training, rotation accuracy decrease drastically for N*xy forall N. Moreover we can note clear accuracy difference between digits, we can understand intuitively some digits are more resistent/distinct even rotated/shifted like 8 and 2, where as 1 is clearly not.

rotate-invariance training: we tried to data-augment with 1*r 2*r 3*r and we see that it increase the accuracy of rotated samples significatively compared to the original (from 81\% to 99\%). This means the original dataset does already a good job at presenting a wide diversity of rotation but we can improve a bit by data-augmenting to add more rotation-invariance.
Moreover these models perform as bad for shift as the original model, which means our augmented dataset doesn't add shift-invariance (as expected) and doesn't lower too much the translation accuracy.
Moreover i+rxy is much better for shift-inv compared to other rotation-trained models but it's worse than any other shift-trained models (even the original one). Which means i+rxy really used a lot of capacity to learn a too wide space of images.
\end{document}
