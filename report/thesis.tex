\documentclass[a4paper,12pt]{report}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage{cite}
\usepackage{hyperref}

\linespread{1.2}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\newcommand{\myname}{Axel Angel}
\newcommand{\mytitle}{Towards Predictable Embedding of Convolutional Neural Networks}
\newcommand{\mysubtitle}{Report}
\newcommand{\mydate}{\today}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\lhead{\myname}
\rhead{\mytitle{} - \mysubtitle{}}
\chead{}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\mydate}

\pdfinfo{
    /Author (\myname)
    /Title (\mytitle)
    /Subject (\mysubtitle)
    }

\newcommand{\eg}{e.g.}


\begin{document}
\thispagestyle{empty}
% TODO: make
First page (make guard, epfl logo, prof names, title, etc)

\newpage
\begin{abstract}
    % TODO: rewrite
    Current research in Computer Vision has shown that Convolutional Neural Networks (CNNs) give state-of-the-art performance in many classification tasks and Computer Vision problems.
    The embedding of CNNs, which is the internal representation produced by the last layer, can indirectly learn interesting topological and relational properties.
    By using a suitable loss function, these models can learn invariance to a wide range of non-linear distortions such as rotation, viewpoint angle or lighting condition.
    In this work, we give useful insights about CNNs embeddings and we propose a new loss function, derived from the contrastive loss, whose mapping under particular distortions is more predictable.
    Given an input image, only a single forward pass is necessary to generate the outputs of all possible distortions, whereas standard models would require computing all combinations of distortions which is too expensive in practice or invariant models would lose the distortion information.
    % ^ TODO: careful about this last statement, too strong!
    Moreover we introduce a simple method to fairly compare quantitatively embeddings whose purpose is to capture topological structures of particular distortions.
\end{abstract}

% ==============================================================================
\tableofcontents


% ==============================================================================
\chapter{Introduction}
common use of high-dim data, like images

dim reduction (LLE, isomap, t-SNE), why, drawbacks

an other approach is to use CNN to classify, then dim reduction on embedding

allows to understand what's happening in CNN, better topology

distortions are spread in the embedding

what about learning invariance? but keep information

need mapping (like DrLIM), train with siamese network

predictable mapping << our contribution

need quantitative measures (to compare) << our contribution

new loss function, short summary of results

use case (medical imaging)

\section{Thesis Outline}
% list chapters with their summarzied content


% ==============================================================================
\chapter{Related Work}
% explain each paper, group them and discuss result adv/drawbacks

More than 20 years ago, researchers discovered that {\em Neural Networks} (NN) is a highly flexible model architecture to solve many classification problems.
Today, many state-of-the-art models use improved NN for computer vision tasks like MNIST \cite{mnist_web} and ImageNet \cite{krizhevsky2012imagenet}, face detection as well \cite{rowley1998neural} and many more \cite{prechelt1994proben1}.

Most of today models are using a {\em Convolutional Neural Network} (CNN) which is a specialized variant for image processing.
This architecture is more efficient for computer vision problems because it constrains the first layers to extract visual cues by using weight-sharing convolutions.

Current researches is mainly focused on more complex classification problems using Deep Learning which suggests to add more layers that can express higher-level features to improve the current results.
The addition of more layers unfortunately require more computations than before but this challenge didn't remained unsolved.
Ways to speed up the training and the classification appeared and were greatly beneficial for the development of this field in recent years\cite{ciresan2011flexible}.
%paper: Flexible, high performance convolutional neural networks for image classification
Researchers and programmers developed various frameworks to train and use NN with different objectives: performance, accessibility or composability.
The most popular are: Theano\cite{bastien2012theano}, Caffe\cite{jia2014caffe}, Torch7\cite{collobert2011torch7}, Pylearn2\cite{goodfellow2013pylearn2}.
We decided to leverage the efficiency and modularity proposed by Caffe for several reasons described later.
%deeper architectures improves accuracy on more complex tasks, people found optimisations to make them fast.

Even though CNN models are now widely used in computer vision and give impressive results, current research exposed the lack of understanding of their behaviors which sometimes leads to unintuitive attacks\cite{szegedy2013intriguing}.
Most of publications doesn't give more formal justifications of their good results because it just works. % FIXME: citation needed?
%paper: Intriguing properties of neural networks
We know that relations between units are complex due do their non-linear nature, this problem is stressed by distortions present in the inputs as well.
Moreover the input-output relation, when inputs are kept distorted on purpose, is usually either killing the signal (invariance) or not in a predictable manner (controlled preservation).

%CNN are complex and strange results were found.
%Research found multiple hypothesises.

% adversial attacks, linearity of networks.
% Towards Deep Neural Network Architectures Robust To Adversarial Examples
% Analysis of classifiers' robustness to adversarial perturbations

The manifestation of this relation can be looked visually using methods to process the output into an other form.
Because we cannot comprehend such a high dimensional space ourself, it is not directly possible look into the output embedding.
Dimensionality reduction methods perfectly fit this description and several methods were applied with different success.

% moved from dim red chapter
Usual methods like PCA, MDS for example, are only modelling linearity between points.
Their primary characteristic is to maximize the variance of the original data which is important for reconstruction but not for visualization.
Other non-linear dimensionality reduction methods, use an optimisation algorithm with a loss function which improve their flexibility for other purpose like visualization.
They preserve local structure/cluster like: isomap, LLE, SNE, (and more).
But the State-of-art discovered SNE has a better potential to keep global structures as well\cite{SNE}.
Most of the methods fails to keep global clusters and local details at the same time\cite{t-SNE}.
%paper: Stochastic Neighbor Embedding

SNE has introduced a better maximization problem to preserve point neighborhood and general clusters with an important emphasis on distances\cite{SNE}.
%Kullback-leibler divergence.
However SNE suffers from: center-crowdedness and difficult optimisation.
That's why papers are reducing CNN embeddings into a 2D human-friendly manifold using t-SNE, which is a slight modification of SNE. % FIXME: more cite?
t-SNE is introduced as a better optimisation formulation and preserve structure at multiple scale, and shown more convincing examples, for example on MNIST\cite{t-SNE} and similar datasets\cite{van2009new}.

Using t-SNE, it becomes possible to see in details how CNN clusters samples depending on multiple factors: essentially based on similarity of the digits (strokes, thickness and shapes).
%Although it is unintuitive, this can be seen by inspecting the embedding.
%we can add constraints against distortions directly over the embedding, to cluster them the way we meant.

Many papers use dimensionality reduction to create a human viewable representation of the output embedding \cite{donahue2013decaf}\cite{yu2014visualizing}\cite{yaotiny}.
Only a few papers actually try to formalize the input-output relations\cite{goodfellow2009measuring}.
Some papers proposes to learn transformation-invariant embeddings, by means of modelling distortions directly into model\cite{gens2014deep} or using data-augmentation\cite{hadsell2006dimensionality}.
%paper: Encoded Invariance in Convolutional Neural Networks
%paper: Deep Symmetry Networks
%proposes an alternative network, symmetry networks, to extend invariance beyond translation by using feature map over arbitrary symmetry groups by use of kernel-based interpolation.
%paper: Dimensionality Reduction by Learning an Invariant Mapping
The latter proposes to creates a dimensionality reducing mapping by training a CNN using a new loss function based on energy models, called the {\em contrastive loss}.
Their results using MINST shows that CNN can learn a mapping that separates labels and group alike digits even with translations.
Moreover they experimented with NORB as well: each image is mapped onto a 3D cylinder whose axises describe the orientation in 2D and the azimuth angle in 1D.
They trained the mapping to be invariant to a very non-linear distortion as well (illumination).
They use an interesting siamese architecture to train pairs of similar sample to be as close as possible.
There are several advantages of such method: the cost penalty is very low and only present in training, the usage of standard CNN allow more freedom for experiments with different architectures and the proposed loss function is effective and simple.

However distortions are damped instead of being quantified in a predictable way.
Moreover they describe qualitatively the coherence of the embedding but doesn't provide nor propose a formal way to measure and compare its quality.


% ==============================================================================
\chapter{Theory: Neural Networks}

\section{NN and CNN Classifier}
%introduce CNN and embeddings

nn and cnn are composed of inter-connected layers of units, or neurons.
to classify an input we fed the first layer of the network.
one layer does a single particular computation over its input, usually by adding a bias then followed by a non-linear function, which is then propagated through its connections to the next layer.
the feedforwarding continues until we reach the last layer which is the output of the network.
usual nn are characterized by: neuron computation is a simple inner product between its input and its own weight followed by a sigmoid; each unit is fully connected to all units in the next layer.
training the weights of such network generally involve gradient descent which minimizes the loss function of the network.
This loss function gives the error between the prediction of the network, its output versus the expected output, which is in classification the class label.
To train, the input is first forwarded into the network to compute the error, then this latter is back-propagated up to the first layer, while each unit weights is adjusted to minimize the unit error.
This process should be repeated until the error on the validation set has converged to a local optimum.

cnn is a special case of nn by adding certain restrictions, working well experimentally with computer vision problems such as image classification.
Three main ideas differentiate them: local receptive fields, weight sharing and subsampling.
in cnn, a convolutional layer can model receptive fields by computing multiple trainable 2D kernels which is convoluted with its entire input whose result is called its feature maps.
We can see a kernel convolution as the replication of a single unit along the dimensions of the input (a 2D grid for images), thus all weights are constrained to be equal by definition.
a convolutional layer contains multiple units where each has its own kernel, thus there are different kernels applied to the image, where a single kernel convolution is called a feature map.
Such layer naturally computes filters which can be seen like feature extractors driven by data.
the benefit of weight sharing in convolutional layers is to reduce the number of global parameters to improve generalization.
usual cnn add a subsampling layer right after a convolutional layer to reduce the dimensionality of the feature maps.
moreover most cnn architectures connects convolutional-subsampling layers to the input which extract features, which is then feed into a regular nn.
Thus we can see a cnn as two parts: a trainable feature extractor which is fed into a processing nn.
meanwhile cnn have more constrained, it has been shown they generally outperforms nn with more invariance in multiple computer vision problems \cite{simard2003best}\cite{mnist_web}\cite{lawrence1997face}\cite{krizhevsky2012imagenet}.

as previously said, researches have shown with t-sne that nn trained for classification have interesting structures in their last-layer embedding.
the choice of the layer is justified by the fact that: deeper layers extract more high-level informations, which is necessary to separate classes, moreover the prediction is a direct product with the last layer, which encourage the network to have a simple structure directly clustered by classes.
that's why this last layer tend to cluster samples of the same class together while separating the rest\cite{donahue2013decaf}\cite{yu2014visualizing}.
thus it is reasonable to evaluate the quality of this embedding as a dimensionality reduction method with great potential to capture more information compared to PCA which captures linear correlations.
it is important to note that t-SNE plays an important role in the quality of this 2D embedding by preserving structures but it should be said also that t-sne cannot separate the original dataset as well as with the help of the classifier features.
Because t-SNE has no knowledge of the labels, it is the classifier who improves the quality of the structures of the final embedding, because it learned specialized filters to distinguish the classes.
This justifies why we combined CNN classifiers with t-SNE to create a 2D embedding for our visualisations.


% ==============================================================================
\chapter{Theory: Dimensionality reduction}
Dimensionality reduction is an important method in machine learning that maps points in a high-dimensional space into a space with a lower number of dimensions.
We will call this lower subspace an {\em embedding}.
Representations suitable for human visualization should keep important relations between points of the original space.
In our experiments, we will apply dimensionality reduction especially on our neural networks' outputs.
This allows us to quickly compare (qualitatively) the impact of controlled distortions applied on images over networks' outputs.
The structure inferred by distances between points such as clusters, intra-cluster neighbors and inter-cluster outsiders reveals important properties of neural network models.
Thus we need accurate low-dimensional representations of embeddings that preserve local distances to compare different models.
In the following sections, we will discuss dimensionality reduction methods that weren't fit for our uses, then we introduce the one we used, called t-SNE, in details.

\section{Optimisation Problem}
Various algorithms differentiate themselves by several properties: their goal (\eg: interpolation, compression, visualization), by what they preserve (\eg: variance, distances), how they model point correlation (\eg: linearly or not) or whether they can model new points (\eg: a representation or a mapping).

First, our choice is guided by our primary need, which is to visualize our data.
Thus the method should produce a 2D or 3D embedding, preferably in 2D for scatter plots easily interpretable.
Secondly, we need a guarantee on the preservation of relations between points, especially the distances keeping them clustered or not.
Considering the previous works we discussed above, t-SNE is the best candidate in the current state-of-the-art.
We now define the method objective functions, probabilities and explain intuitive properties.

%introduce theory of SNE, then t-SNE modification.
Let's define input space $X = \left\{ x_1, x_2, \dots, x_n \right\}$ then the goal is to map into a lower-space $Y = \left\{ y_1, y_2, \dots, y_m \right\}$, where $n$ is very high (around a few hundreds to thousands, \eg: mnist pixels dimensions: 784) and $m$ is $2$ or $3$.
SNE uses two Gaussian distributions for each point expressing the neighbor distances in $X$ and its equivalent in $Y$.
The Kullback-Leibler divergence is used to compute the objective function, which represents the mismatch of these two distribution for each pair:
\begin{eqnarray}
    L = \sum_i KL(P_i || Q_i) = \sum_i \sum_j p_{j|i} \log\left(\frac{p_{j|i}}{q_{j|i}}\right)
\end{eqnarray}
The probability for a pair of point $x_i$ electing $x_j$ in $X$ follows a Gaussian is as follow:
\begin{eqnarray}
    p_{j|i} = \frac{\exp(-|x_i - x_j|^2 / 2 \sigma_i^2)}{\sum_{k \not = i} \exp(-|x_i - x_k|^2 / 2 \sigma_i^2 )}
    %&
    %q_{j|i} = \frac{\exp(-|y_i - y_j|^2)}{\sum_{k \not = i} \exp(-|y_i - y_k|^2)}
\end{eqnarray}
The equivalent probability $q_{j|i}$ in $Y$ is the same except that $\sigma = 0$ and $x$ is replaced by $y$.
Therefore, a closer pair implies a higher neighbor-election probability because the distance is low.
This cost function gives an asymmetrical importance to the distances: nearby points in $X$ are greatly penalized if they are far in $Y$; whereas a small cost is incurred for pairs far in $X$ but close in $Y$.
%There is a simple physical interpretation of the SNE optimisation problem: pairs are modeled by asymmetrical springs in a mechanical system, the best solution is when the system is still.

As we said SNE suffers from crowdedness problems in the middle of the embedding and the optimisation is harder due to the asymmetrical nature of the objective function.
Both problems were addressed in t-SNE which give very good results in practice.
The two major differences with t-SNE is the symmetrization of the cost function and replaces the distributions of the embedding by student variants.
In t-SNE, a single objective function is minimized:
\begin{eqnarray}
    L = \sum_i KL(P || Q) = \sum_i \sum_j p_{ij} \log\left(\frac{p_{ij}}{q_{ij}}\right)
\end{eqnarray}
where:
\begin{eqnarray}
    p_{ij} = \frac{p_{j|i} + p_{i|j}}{2 n}
\end{eqnarray}
which forces outliers points to contribute more to the loss.
And:
\begin{eqnarray}
    q_{ij} = \frac{1 / (1 + |y_i - y_j|^2)}{\sum_{k \not = l} 1/(1 + |y_k - y_l|^2)}
\end{eqnarray}
which replaces the $q$ distribution in SNE by a Student with a heavier tail: distances in high dimensional spaces spread across more dimensions; in low dimensional space, the accurate equivalent distance needs to be much higher per dimension (thus more points end up farer in $Y$, a heavier tail than in $X$).

As before, a point cannot elect itself: $p_{ii} = q_{ii} = 0$ and probabilities are symmetric for both distributions: $p_{ij} = p_{ji}$ and $q_{ij} = q_{ji}$.
As stated previously the input space $X$ has much more dimensions were distances can be expressed than the 2 dimensions of $Y$.
In summary, t-SNE uses Kullback-Leibler divergence to minimize the mismatch of these two spaces by means of probabilities, therefore the chance of important local structures (frequent patterns) being preserved is higher than with other methods (mosts don't express this goal through an objective function).
Global structures is encouraged by the coherence of the local structures as the divergence decreases and the system stabilizes.

\section{CNN for Dimensionality Reduction}
Usual dimensionality reductions like t-SNE are helpful for many visualizing tasks but it has important drawbacks as well.
We think the most important ones are: the computational cost, the incapacity of mapping new points and the indirect control over the resulting embedding.
Current implementations of t-SNE are still rare, unpolished and require tricks to make them tractable in practice (\eg: reducing first with PCA).
Fortunately there are new promising alternatives emerging directly harnessing the power of NN.
We introduce such models in the following section.

%there is a direct way to learn an embedding with CNN without t-SNE.
CNN are mostly used for classification but we can optimize them for other purposes as well.
Instead of the softmax loss function used for classification, we can use a special training architecture with a suitable loss that when optimized tries to satisfy some constraints.
For example to create an embedding with chosen properties directly found into the output layer.
Moreover the ideas presented by reduction methods like t-SNE can be formulated in different terms to be applicable to NN.
In our case the most important idea is to keep similar points together and dissimilar far.

One solution is the siamese network with a contrastive loss which is used only for the optimisation phase.
The siamese architectural idea is to create two weight-sharing instances of a network where a pair of two samples are fed at once, one per network.
The distance between the two outputs is used to compute the error against the label for the pair.
when the training is complete, a single instance of this network is used to feed an input which gives the dimensionality-reduced point.
% FIXME: figure.
In this architecture, a labels describes whether or not a pair should be close in the embedding.

The contrastive loss function is based on energy models but a single attractive term is not sufficient because it would allow degenerate solutions where all points squashed together; thus an opposing term to push dissimilar pairs should appear as well.
The definition of the contrastive loss for a pair is as follow:
\begin{eqnarray}
    L = \frac{1}{2} Y (D_W)^2 + \frac{1}{2} (1-Y) \max(0, m - D_W)^2
\end{eqnarray}
where $Y \in \{0,1\}$ is the label: $1$ for similar pairs, $0$ otherwise; $D_W \in \R^N$ is the difference between the two network outputs and the parameter $m \in \R$ defines the minimal distance between dissimilar points.

The definition of similarity is left open for the application at hand.

\section{CNN for Predictable Reduction}
the method above can learn an embedding which tries to pair the way it was intended, but there is still room for improvement.
First, the structure of the resulting embedding is determined by the dataset and again we indirectly shape it by giving pairing information which helps to constraint local structures.
However, there is no guarantee that this system will converge to an intended global structure, for example having certain deformations in a predictable ordering (\eg: from lowest to highest) or shape (\eg: a line, plane or circle).
Secondly, the training required to make the embedding may converge to a desirable structure after a long time or never even with multiple run using different seeds due to the lack of constraint.
The number of embedding dimensions, $n$, is higher than the dimension of the pairing, $1$, which is why such model is not predictable, because these dimensions gives more freedom to the model in the way to represent these pairs arbitrarily.

In the intend to improve this solution, a possibility is to add more constraints directly into the optimisation process.
We propose to give more informations for each pair which can be used in the loss function to create these enforcements.
This method allows to control separately the usage of each dimension of the embedding to express different properties of our dataset.
To achieve this goal, we generalize the contrastive loss to pairs having $n$ pairing labels represented into an $N$-dimensional embedding, where $n \leq N$ by definition.
The relation between a pairing label and its binds to certain dimensions, should be defined per a use-case basis, as the pair relation.

We now introduce our loss function with a formal definition.
Let's define $N$ as the number of embedding dimensions, $n$ the dimensions of the labels, where $n \leq N$ as before.
Let's define $D \in \N^n$ as the number of embedding dimension assigned per label, then we restrain ourself to: $\sum_i^n D_i = N$, which states each dimension should be assigned to a single label.
The definition of the generalized $n$-dimensional contrastive loss for a pair is as follow:
\begin{eqnarray}
    L = \frac{1}{2} \sum_{i=0}^n \left( Y_i (D_{Wi})^2 + (1-Y_i) \max(0, m_i - D_{Wi})^2 \right)
\end{eqnarray}
where $Y \in \{0,1\}^n$ with $Y_i$ being the $i$th component of $Y$, $D_{Wi} \in \R^{D_i}$ is the difference between the two points in the sub-embedding for dimensions of the $i$th component, and $m_i \in \R$ is the minimal distance for dimension $i$.


% ==============================================================================
\chapter{Methodology}

explain our models.
lenet archi.
layers, parameters.
add figure.
we cut the last layer, give same archi as paper.

use of Caffe.
give arguments why.
how we used caffe: built-in solver.
use python to feed-forward, test.

developed own web plotter for interactive.
display sample images, filtering/highlights capabilities, zoom, move.
good insight and feedback of results.

introduce datasets: mnist and norb.
mnist use lenet archi.

how we create our train/test dataset
python script, predictable.
pairing method.
reproduce lecun mnist and norb.
we use 3d for mnist, compare with lecun project 2d.

how are models trained in caffe.
guideline to reproduce, parameters and stuff.
give for mnist and norb.

contribution: how we can compute our energy-distance for comparisons.


% ==============================================================================
\chapter{Results and Discussion}

tables and graphs: loss of training (for major models), add accuracy for comparison, compare lecun with ours (energy-distance).

\section{t-SNE on LeNet}
explain what we found using t-SNE on last layer of lenet, on mnist.

people add distorted samples to their dataset (data augmentation) so models can learn to be transfo invariant, but results shows the embedding cluster them into clusters per transfo instead of per label [XXX].

\section{Contrastive LeNet}
explain what we found using double-contrastive loss with lenet, on mnist.

can add norb, when finished.


% ==============================================================================
\chapter{Conclusion}

% bibliography
\bibliography{thesis}{}
\bibliographystyle{plain}

% required by NORB dataset
\nocite{lecun2004learning}

\end{document}
