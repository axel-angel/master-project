\documentclass[10pt]{beamer}
\usetheme{Berlin}
\usecolortheme{lily}
\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{tikz}
\beamertemplatenavigationsymbolsempty

\usepackage{pgfpages}
\setbeameroption{show notes}
\setbeameroption{show notes on second screen=right}

\newcommand{\docauthor}{Axel Angel}
\newcommand{\doctitle}{Towards Distortion-Predictable Embedding of Neural Networks}
\newcommand{\docsubtitle}{Defense}
\newcommand{\eg}{e.g.}
\newcommand{\reff}[1]{~\ref{#1}}
\setkeys{Gin}{width=1.0\textwidth}

\author{\docauthor}
\title{\doctitle \\ \docsubtitle}

\pdfinfo{
    /Author (\docauthor)
    /Title (\doctitle)
    /Subject (\docsubtitle)
    }

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\note{\begin{itemize}
    % Summary (recap what said during presentation)
    \item present an example of application for our method
    \item brief recap of CNN
    \item found t-SNE is good for visualization
    \item not suitable for predictable embeddings
    \item generalized the contrastive loss
    \item provide comparisons in qualitative and quantitative forms
    \item several directions for future works
    \item final thoughts and conclude this work
\end{itemize}}

\tableofcontents{}

\section{Motivations}
\begin{frame}
    \frametitle{Motivations}
    \begin{itemize}
        \item Face recognition
        \item Develop the theory behind it
        \item Our work is a start
    \end{itemize}


    \note{\begin{itemize}
        \item there are examples of applications for our method.
        \item ``face recognition'': pose estimation, expression, mouth, identity
        \item benefit from predictable embeddings.
        \item face recognition is too hard to start
        \item we experimented with simpler datasets (with mnist and norb)
    \end{itemize}}
\end{frame}

\section{Review of CNNs}
\begin{frame}
    \frametitle{Review of CNNs}
    \begin{itemize}
        \item Architecture
        \item Similar to a blackbox
        \item We focus on the embedding
    \end{itemize}


    \note{\begin{itemize}
        \item architecture:
            \begin{itemize}
                \item built-in feature extractor (ConvNets)
                \item built-in classifier (NeuralNets)
                \item forward/backward SGD
            \end{itemize}

        \item unintuitive properties:
            \begin{itemize}
                \item adverserial attacks
                \item conditions of good convergence
                \item unpredictable embedding
            \end{itemize}
        \item this work focus predictability of embedding under transformations.
        \item can make them learn certain properties
    \end{itemize}}
\end{frame}

\section{Visualization with t-SNE}
\begin{frame}
    \frametitle{Visualization with t-SNE}
    \begin{itemize}
        \item Definition of t-SNE
        \item Optimized properties (spring model)
        \item Previous works
    \end{itemize}


    \note{\begin{itemize}
        \item define t-SNE: reduce dataset from nD to 2D, with optimisation problem.
        \item original points are Gaussian, new points are Student distr.
        \item optimisation make the new repr matches the original: based on points distances
        \item how we did: train/test MNIST, behead last layers and reduce with t-SNE
        \item many papers showed this already but few worked on distortions
        \item kind of distortion: translations, rotations.
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Visualization with t-SNE}
    % FIXME: show figure
    \begin{itemize}
        \item Results are unexpected
        \item We could adapt t-SNE
    \end{itemize}
    However there exist better alternatives.


    \note{\begin{itemize}
        \item discontinuities, questionable clustering
            \begin{itemize}
                \item along some transfo, esp translation
                \item clusters are dominated by distortions (instead of digits)
                \item clusters are mixed up, unclear/confusing
            \end{itemize}
        \item limitations
            \begin{itemize}
                \item lack of control (opti problem is unlabeled)
                \item cannot add points, recompute from scratch, costly
            \end{itemize}
        \item fails to provide concluding results
        \item could reformulate/adapt t-SNE but better alternative exists
        \item what we show in the following
    \end{itemize}}
\end{frame}

\section{Predictable embeddings}
\begin{frame}
    % explain concepts
    \frametitle{Predictable embeddings}
    Learn pair relations directly, by using:
    \begin{itemize}
        \item Neural networks (CNN or NN)
        \item Siamese architecture
        \item Contrastive loss
    \end{itemize}
    %many pairing relations can be learned directly into embeddings.
    % FIXME: show figure
    % FIXME: show formula contrastive loss


    \note{\begin{itemize}
        \item neural networks can express complex non-linear mappings
        \item learn relation: mapping of pairs (2 images) to 1/0 similarity
        \item previous work {\em Dimensionality Reduction by learning an invariant mapping} (DrLIM) by: Raia Hadsell, Sumit Chopra, Yann LeCun
        \item showed: neural networks can learn structured embeddings
        \item showed: can be used for dimensionality reducing
        \item training with: siamese and contrastive for loss (also used for face recognition, same authors)
        \item trained networks to represent: MNIST by similarity in 2D, camera viewpoint in 3D
        \item points similar = close together, similar to t-SNE
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Predictable embeddings}
    In our work we provide:
    \begin{itemize}
        \item An extension of this loss for $N$ relations
            % FIXME: show formula our contrastive loss
        \item Qualitative and quantitative results
    \end{itemize}


    \note{\begin{itemize}
        \item our work is based on the mentioned paper
        \item we reuse the same concepts but extend contrastive loss
        \item our goal is: learn more similarities, separately, add more control
        \item can decide which dimensions express what similarity (align to axes)
        \item plots serve as qualitative results for human to visually compare
        \item but we also introduce quantitative measures to compare with previous work (reproduce DrLIM work)
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Predictable embeddings}
    % FIXME: show our figure + DrLIM figure (MNIST)
    \begin{itemize}
        \item Multiple relations at once
    \end{itemize}


    \note{\begin{itemize}
        \item quick recap dataset MNIST
        \item on this experiment MNIST, we train with two classes like DrLIM
        \item learn to put similar numbers together
        \item DrLIM do it in 2D, we do the same but add a new dimension
        \item it serves to express translations without disturbing the other 2D
        \item models can learn >1 relations: correctly (quality) and effectively (1 model)
        \item quality: qualitative measures (in 2D) show it's as good and we cannot tell them apart (good coherence inside clusters)
        \item effectively: 1 model = train 1 model, share weights/features for both tasks
        \item we also tried on non-linear transfo (rotations) with same results (similar clustering).
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Predictable embeddings}
    % FIXME: show our figure + DrLIM figure (NORB)
    \begin{itemize}
        \item More predictable structures
    \end{itemize}


    \note{\begin{itemize}
        \item quick recap dataset NORB
        \item on this experiment NORB, same task DrLIM
        \item found our solution easier to work with (converge to correct shape)
        \item cylinder: cyclic structure for azimuth, ordered value for elevation
        \item major difference: alignment with axes, points are more separated.
        \item quality seems better (wider radius, more separated) but our quantitative measure doesn't show that
    \end{itemize}}
\end{frame}

\section{Conclusion}
\begin{frame}
    \frametitle{Future works}
    \begin{itemize}
        \item Make experimental comparisons with regression
        \item Show predictability with auxiliary networks
        \item Fields applications: Face Recognition, Bio-Medical Imaging.
    \end{itemize}

    \note{\begin{itemize}
        \item models can learn multiple pairing at once: MNIST (similarity + class + transfo), NORB (azimuth + elevation)
        \item like regression but major differences (abs/relative pos + hard/soft margins); absolute position is not easily defined in applications like face reco where (eg face expression: smile, sad, chose value? Let the model find what's good); future work should compare our solution to regression on MNIST (more constraints help?)
        \item direct predictability, train a model to predict other-transfo features given one-transfo features (ex: transl 0 -> trans N for all N); the auxiliary network learn to map features of a particular transfo to all other transfo (accuracy = measure of pred)
        \item future work can show how our solution works with fields ; like face reco (multiple face features: pose, expression, mouth, eyes, eyebrows, â€¦); like bio medical (instead of all costly rotation of 3D images -> features, compute directly independent features with its rotation).
    \end{itemize}}
\end{frame}

% like summary
\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Motivations of our work
        \item Unexpected results using t-SNE
        \item Proposition of an alternative with NN
        \item Further directions are promising
    \end{itemize}

    \note{\begin{itemize}
        \item began motivation with applications (face reco, bio imaging)
        \item small overview NN (their unintuitive properties, black box)
        \item we explore one particular aspect: embeddings predictability
        \item first try with t-SNE unexpected results, too many limitations
        \item we decided go to pure NN solutions (aspects of data directly into embedding)))))
        \item previous work already proposed solutions for 1 relation
        \item we wanted: more information to be encode and making sure we have a more predictable result by adding more constraints on dimensions
        \item we extended the contrastive loss to allocate dimensions per relation (more control)
        \item our good results demonstrated the practicality of our solution on two datasets
        \item however our work is only a beginning: measures should be defined to capture predictability and experiments should try our solution on practical applications.
    \end{itemize}}
\end{frame}

\end{document}
