[file]
pres.pdf
[notes]
### 1
welcome, present my master project
goal/why: study/better understand CNN, embedding (intern repr auto learn from data)
add more control to embedding -> interpretable
method/how: extend previous state-of-the-art dim-red for desired property
### 2
example of application (face reco, charact -> feats)
brief recap of CNN (archi, train, good CV)
goal: control embedding
 (1) t-SNE good visualization (definition, results, limitations)
 (2) alternative: pure NN, extend state-of-the-art (contrastive loss)
several directions for future works and final thoughts
### 3
there are examples of applications for our method.
"face recognition": feed NN, today: blackbox features; our: make predictable, explain figure
pose estimation, expression, identity
but we started simpler problem, like digits and 3D objects, translations, rotations, viewpoint
### 4
architecture:
  feat extr: conv, pool, visual cues (noise)
  fully conn, deep, complex, features/embedding, proba
  train, image/label, fwd/bwd, update weights, prediction
properties not well understood:
  why converge, why good, when?
  features repr not real meaning/control
we focus study embedding, add constrain, can make them learn certain properties on feats/output when the input is distorted (transl/rotat)
### 5
first, why t-sne? state-of-the-art dim-red, definition: reduce nD to 2D, optimisation problem.
t-sne on NN features high-dim, make 2D repr keep property = points distances (far/close), visualize
already done (mnist), good to find clusters, unsupervised
### 6
1st exp: handwritten digit: mnist (~60k images, 0-9 classes)
cluster=class well separated, similar=close
many papers already but few worked with distortions (translations, rotations) to study embedding/feat repr: how feature change under transfo
### 7
2nd exp: 1 class, add transfo (translations, rotations)
discontinuities, inconsistent (esp. transl)
  clusters are dominated by distortions (instead of digits)
  clusters are mixed up, inconsistent
limitations
  lack of control (unsupervised)
  cannot add points, recompute scratch, costly
reformulate/adapt t-SNE? Better alternative
### 8
neural networks: complex non-linear mappings
previous work: DrLIM instead of classify class, learn mapping (dim-red), invariant property
NN learn structured emb supervised, figure, MNIST, property? ignore transl (invariant)
repeat: it's embedding, not t-SNE
structures: MNIST by similarity in 2D, NORB camera viewpoint in 3D
points similar = paired = close together, like in t-SNE
### 9
how: train batches of 2 images to learn parity (labels areâ€¦)
siamese: 2 instances of NN (mapping), X1,X2 = 2 images
compute features, each instance (features)
loss on distances between 2 features
### 10
contrastive for loss
paired points = close, unpaired = far
don't penalize very far unpaired (margin, SVM)
also used for face recognition, same authors
### 11
extend, why: to add more control for our goal
goal: learn multiple charact, control alloc dimensions separately: decide which dimensions in emb express what charact
efficient: 1 model, many charact, share weights
how: reuse same concepts but generalize, formula diff, give example p=3 (class/transl/rotate)
plots = qualitative (human visually compare), no real prev work
### 12
our: 3D emb, fig 2project: left 2D (similarity) + 1D (translation vertical)
DrLIM particular case (left same) + add info, don't disturb 2D
explain figure, left/right, good clusters = easy to interpret
fact: also non-linear transfo (rotations) success (similar clustering).
### 13
qualitative inspection, performance very similar
hard to compare to DrLIM because different
can project on some components to compare with DrLim (particularity our approach).
quantitative measure is common loss (same params)
### 14
quick recap dataset NORB
on this experiment NORB, same task DrLIM, compare
found our solution easier to work with (converge better cylinder)
cylinder: 2D cyclic structure for azimuth, 1D ordered value for elevation
major difference: quality seems better, more separated, axes alignement (assign charact to 2D/1D separate)
### 15
plot: but quantitative measure doesn't show that
figure: show which
our 2 models,
higher loss but quality better, fail to capture
why higher loss
DrLIM m=10 then fail to converge, like paper, cylinder
### 16
figure: 2 projections for our embedding/model
left: elevation in lines
right: consistency of azimuth around cyclic shape
### 17
models can learn multiple pairing at once: MNIST (sim/class/transfo), NORB (azi/elev)
future work compare to regr, similar but major differences (hard constr pos+marg), 
direct predictability, train a model to derive features of distorted, from a non distorted sample, accu=quantify
future work apply fields (face reco/bio medical rotations)
### 18
began motivation applications, small overview NN (unexplained, black box)
explored one particular aspect (embeddings predictability)
first try with t-SNE unexpected results, too many limitations
then pure NN (aspects of data directly into embedding)
previous work already 1 pairing, wanted more info/control -> predictable (constraint on dimensions)
extended contrastive to allocate dimensions per relation (more control, share weights, efficient)
our good results demonstrated the practicality of our solution on two datasets, hard to compare, no previous related work, need better measure
hope encourage research to continue where we stopped: harder problem (face reco)





### 19
we will release code (train/model) and tools (plot) on github.

Thank you for your attention.


