\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\lhead{Axel Angel}
\rhead{Properties of Convolutional Neural Networks}
\chead{Report}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\lfoot{\today}

\pdfinfo{
    /Author (Authors)
    /Title (Title)
    /Subject (Subject)
    }

\begin{document}
Write about results, raw sketch to complete.

why chose caffe? because easy, various models available, optimized and flexible.

papers use pca before t-sne, sklearn recommends too, in practice it works well.

some papers found that cnn models are easily fooled by adversial noise, because too linear? because image space too wide?

some papers can train using adversial examples. mnist dataset is not resistant to many generations of adversial training. imagenet works better (because natural images?)

we need a way to formalize invariance, resitance to adversial noise, deformations. present results and compare models, attacks and present numbers.

lenet on mnist can be easily fooled by translations, because dataset is heavily normalized, translation-invariance is weak. small rotations works well but can fool easily. seems invariant to contrast (invariant filters?).

interactive tool findings? discontinuities in the output, we can fool it with certain angles. we see the receptive field by shifting, after some margin it goes non-sense

transfo tracer findings? how to plot better? we find pattern in the t-SNE. Lines for positive angles, a curve for negative angles, does it mean anything since its t-SNE? invariant transformations? blur has small impact, why? shift is important after a certain threshold (margin? recetivity field size?), why? can we relate this to the CNN-code: easy, hard?
\end{document}
